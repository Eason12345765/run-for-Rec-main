{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19337ea1-f8d7-4705-a42d-d5b4f9cdc5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pyspark in /root/miniconda3/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.12/site-packages (3.4.2)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.12/site-packages (1.15.2)\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.12/site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: texttable in /root/miniconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /root/miniconda3/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /root/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /root/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /root/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /root/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /root/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /root/miniconda3/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /root/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyspark pandas networkx scipy torch texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f88425-aabb-45c0-a102-2f97620791a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bb1f5a-1729-4325-bd11-c64379c705f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/24 17:35:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Exception in thread \"main\" org.apache.spark.SparkException: Failed to get main class in JAR with error '/root/target_dir/JobRec-main (Is a directory)'.  Please specify one with --class.\n",
      "\tat org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:1052)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:533)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "start_date = \"2023-06-03\"\n",
    "end_date = \"2023-06-30\"\n",
    "path = \"/root/target_dir/JobRec-maintarget_dir/JobRec-main/job\"\n",
    "city_short = \"bj\"\n",
    "\n",
    "command = [\n",
    "    \"spark-submit\",\n",
    "    \"--master\", \"local[*]\",\n",
    "    \"/root/target_dir/JobRec-main\",\n",
    "    start_date,\n",
    "    end_date,\n",
    "    path,\n",
    "    city_short\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe33506f-68cf-4407-a6e8-6316e9f7fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java安装成功\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['apt-get', 'update'], capture_output=True, text=True, check=True)\n",
    "    result = subprocess.run(['apt-get', 'install', '-y', 'openjdk-8-jdk'], capture_output=True, text=True, check=True)\n",
    "    print(\"Java安装成功\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Java安装失败: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1984e40b-bb2c-4fa2-8db6-3191f054c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "java_path = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"  # 请根据实际输出修改\n",
    "os.environ[\"JAVA_HOME\"] = java_path\n",
    "os.environ[\"PATH\"] = f\"{java_path}/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f07950e-bff5-4c05-86d5-62011b24d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates tain: ['2023-06-03', '2023-06-04', '2023-06-05', '2023-06-06', '2023-06-07', '2023-06-08', '2023-06-09', '2023-06-10', '2023-06-11', '2023-06-12', '2023-06-13', '2023-06-14', '2023-06-15', '2023-06-16', '2023-06-17', '2023-06-18', '2023-06-19', '2023-06-20', '2023-06-21', '2023-06-22', '2023-06-23', '2023-06-24', '2023-06-25']\n",
      "dates test: ['2023-06-26', '2023-06-27', '2023-06-28', '2023-06-29', '2023-06-30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/24 17:35:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------+---------+-----------------+-----------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+---------------+----------+\n",
      "|   job_id|update_time|city_name|city_code|position_lv3_code|position_lv3_name|low_salary|high_salary|degree_code|experience_code|             title|days_per_week_u|area_city_code_u|area_district_u|      date|\n",
      "+---------+-----------+---------+---------+-----------------+-----------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+---------------+----------+\n",
      "|288845965| 1685614345|     上海|101020100|           180807|       量化研究员|       400|        700|        203|            109|高频因子挖掘实习生|              4|             021|         长宁区|2023-06-12|\n",
      "+---------+-----------+---------+---------+-----------------+-----------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+---------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- job_id: string (nullable = true)\n",
      " |-- update_time: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- position_lv3_code: string (nullable = true)\n",
      " |-- position_lv3_name: string (nullable = true)\n",
      " |-- low_salary: string (nullable = true)\n",
      " |-- high_salary: string (nullable = true)\n",
      " |-- degree_code: string (nullable = true)\n",
      " |-- experience_code: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- days_per_week_u: string (nullable = true)\n",
      " |-- area_city_code_u: string (nullable = true)\n",
      " |-- area_district_u: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occ_train cnt: 1514796\n",
      "occ_test cnt: 370088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job pos cnt: 126\n",
      "['100506', '100403', '101407', '100404', '100804', '100124', '100817', '101308', '101011', '101017', '100706', '101100', '100301', '100210', '101404', '101009', '100701', '100311', '100807', '101301', '101299', '101006', '100116', '101001', '101002', '100601', '101003', '100209', '100801', '100904', '101302', '101309', '100816', '100121', '100805', '101018', '100202', '100405', '100305', '101202', '100606', '101312', '100704', '100205', '100410', '101306', '100302', '100104', '100212', '101008', '101013', '101016', '100105', '100107', '101015', '100806', '100310', '101010', '101406', '101310', '100811', '100199', '100406', '101307', '101007', '100303', '101311', '100122', '100407', '100307', '100109', '101101', '100106', '100401', '101201', '100118', '100705', '101014', '100306', '100703', '100803', '100208', '100304', '100511', '100808', '100512', '100810', '100103', '100114', '100117', '101402', '100402', '100101', '100408', '100308', '101005', '100203', '100409', '100309', '100603', '101403', '100802', '101305', '100123', '100102', '100607', '101408', '100605', '100813', '100115', '101405', '100707', '100507', '100901', '100809', '100508', '101012', '101401', '101004', '100702', '100120', '100211', '100299', '100604', '100818', '100108']\n",
      "job 数值化1\n",
      "\tNULL field process...\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+\n",
      "|   job_id|city_code_str|city_name|update_time|position_code|low_salary|high_salary|degree_code|experience_code|              name|days_per_week_u|area_city_code_u|      date|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+\n",
      "|288845965|    101020100|     上海| 1685614345|       180807|       400|        700|        203|            109|高频因子挖掘实习生|              4|              21|2023-06-12|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+\n",
      "|   job_id|city_code_str|city_name|update_time|position_code|low_salary|high_salary|degree_code|experience_code|                name|days_per_week_u|area_city_code_u|      date|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+\n",
      "|286718149|    101010100|     北京| 1684827552|       180118|        15|         18|        204|            101|Coverage Officer ...|              5|              10|2023-06-27|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "job 数值化2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+---------+---------+\n",
      "|   job_id|city_code_str|city_name|update_time|position_code|low_salary|high_salary|degree_code|experience_code|              name|days_per_week_u|area_city_code_u|      date|name_code|city_code|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+---------+---------+\n",
      "|288845965|    101020100|     上海| 1685614345|       180807|       400|        700|        203|            109|高频因子挖掘实习生|              4|         10021.0|2023-06-12|  82672.0|      1.0|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+---------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+---------+---------+\n",
      "|   job_id|city_code_str|city_name|update_time|position_code|low_salary|high_salary|degree_code|experience_code|                name|days_per_week_u|area_city_code_u|      date|name_code|city_code|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+---------+---------+\n",
      "|286718149|    101010100|     北京| 1684827552|       180118|        15|         18|        204|            101|Coverage Officer ...|              5|            10.0|2023-06-27|  41973.0|      0.0|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+---------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "job 数值化3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+---------+\n",
      "|   job_id|city_code_str|city_name|update_time|position_code|low_salary|high_salary|degree_code|experience_code|              name|days_per_week_u|area_city_code_u|      date|name_code|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+---------+\n",
      "|288845965|    101020100|     上海| 1685614345|       180807|       400|        700|          5|              0|高频因子挖掘实习生|              4|         10021.0|2023-06-12|  82672.0|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+------------------+---------------+----------------+----------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+---------+\n",
      "|   job_id|city_code_str|city_name|update_time|position_code|low_salary|high_salary|degree_code|experience_code|                name|days_per_week_u|area_city_code_u|      date|name_code|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+---------+\n",
      "|286718149|    101010100|     北京| 1684827552|       180118|        15|         18|          6|              0|Coverage Officer ...|              5|            10.0|2023-06-27|  41973.0|\n",
      "+---------+-------------+---------+-----------+-------------+----------+-----------+-----------+---------------+--------------------+---------------+----------------+----------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-------------+----------+-----------+-----------+---------------+---------------+----------------+----------+---------+\n",
      "|   job_id|city_name|update_time|position_code|low_salary|high_salary|degree_code|experience_code|days_per_week_u|area_city_code_u|      date|name_code|\n",
      "+---------+---------+-----------+-------------+----------+-----------+-----------+---------------+---------------+----------------+----------+---------+\n",
      "|288845965|     上海| 1685614345|        80807|       400|        700|          5|              0|              4|         10021.0|2023-06-12|  82672.0|\n",
      "+---------+---------+-----------+-------------+----------+-----------+-----------+---------------+---------------+----------------+----------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+---------+-----------+-------------+----------+-----------+-----------+---------------+---------------+----------------+----------+---------+\n",
      "|   job_id|city_name|update_time|position_code|low_salary|high_salary|degree_code|experience_code|days_per_week_u|area_city_code_u|      date|name_code|\n",
      "+---------+---------+-----------+-------------+----------+-----------+-----------+---------------+---------------+----------------+----------+---------+\n",
      "|286718149|     北京| 1684827552|        80118|        15|         18|          6|              0|              5|            10.0|2023-06-27|  41973.0|\n",
      "+---------+---------+-----------+-------------+----------+-----------+-----------+---------------+---------------+----------------+----------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+--------------------+----------+\n",
      "|   job_id|            features|      date|\n",
      "+---------+--------------------+----------+\n",
      "|288845965|[80807.0,400.0,70...|2023-06-12|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+--------------------+----------+\n",
      "|   job_id|            features|      date|\n",
      "+---------+--------------------+----------+\n",
      "|286718149|[80118.0,15.0,18....|2023-06-27|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "数据归一化\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+---------+--------------------+----------+--------------------+\n",
      "|   job_id|            features|      date|      scaledFeatures|\n",
      "+---------+--------------------+----------+--------------------+\n",
      "|288845965|[80807.0,400.0,70...|2023-06-12|[1.72827604337581...|\n",
      "+---------+--------------------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+--------------------+----------+--------------------+\n",
      "|   job_id|            features|      date|      scaledFeatures|\n",
      "+---------+--------------------+----------+--------------------+\n",
      "|286718149|[80118.0,15.0,18....|2023-06-27|[1.71353991663077...|\n",
      "+---------+--------------------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "KMeans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = -0.17320950653382725\n",
      "Cluster Center\n",
      "[1.66373167e-02 4.80750880e-02 5.76854434e-02 2.94378618e+00\n",
      " 0.00000000e+00 3.33838669e+01 7.05860601e-03 1.84688550e-02]\n",
      "500\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+-------------+-----------+-----------+-----------+---------+---------+------------+-------------+\n",
      "|   job_id|      date|prediction|position_code| low_salary|high_salary|degree_code| exp_year| workdays|   area_code|    name_code|\n",
      "+---------+----------+----------+-------------+-----------+-----------+-----------+---------+---------+------------+-------------+\n",
      "|288845965|2023-06-12|       276|     1.728276|  1.1055187|  1.3568841|  2.9437861|      0.0|26.707094|  0.08355995|    1.6338236|\n",
      "|288862726|2023-06-12|       226|    4.0684543| 0.01658278|0.013568841|        0.0|      0.0|33.383865|8.3384846E-5|     3.454741|\n",
      "|283438444|2023-06-12|         2|  0.008662019|0.030401763| 0.04264493|  2.9437861|1.4627733|33.383865|         0.0| 0.0043477984|\n",
      "|280258590|2023-06-12|       115|  0.002160158|0.022110373|0.023260871|  2.9437861|1.4627733|33.383865|   0.8367753|1.18576325E-4|\n",
      "|289381991|2023-06-12|       421|   0.21603718| 0.04974834|0.054275364|  2.9437861|1.4627733|33.383865|8.3384846E-5|    3.1922128|\n",
      "|267444768|2023-06-12|       117|  0.004512805|0.055275932| 0.07753623|  2.3550289|1.4627733|33.383865|8.3384846E-5|    1.5106031|\n",
      "|159626734|2023-06-12|         2|  0.010864953| 0.06909492| 0.09692029|  2.9437861|1.4627733|33.383865|8.3384846E-5| 2.3715265E-4|\n",
      "|278658879|2023-06-12|        44|    0.8619458| 0.03316556|0.038768116|  2.3550289|      0.0|33.383865|8.3384846E-5|   0.85914475|\n",
      "|289012488|2023-06-12|        78|   0.01927032|0.035929356|0.034891304|  2.9437861|1.4627733|33.383865|8.3384846E-5|    0.6274861|\n",
      "|130890904|2023-06-12|       117|  0.025686631| 0.04145695|0.048460145|  2.3550289|1.4627733|33.383865|8.3384846E-5|    1.2902685|\n",
      "|284101269|2023-06-12|       113|    4.2841706|0.027637966|0.029076088|  2.9437861|1.4627733|33.383865|         0.0|    2.3070407|\n",
      "|105060520|2023-06-12|       383|  0.017174324|0.027637966|0.021322465|  2.9437861|1.4627733|33.383865|  0.41951752|  0.006067155|\n",
      "|179239653|2023-06-12|       317|   0.86200994| 0.03316556|0.046521742|  2.3550289|1.4627733|33.383865|8.3384846E-5|    2.5772958|\n",
      "|238953304|2023-06-12|       322|  0.002160158| 0.03316556|0.038768116|  2.9437861|0.4875911|33.383865|8.3384846E-5| 0.0068774265|\n",
      "|253116739|2023-06-12|         0|  0.025686631| 0.04145695|0.058152176|  2.9437861|      0.0|33.383865|8.3384846E-5| 0.0038734933|\n",
      "|290854545|2023-06-12|       200|  0.008640632|0.077386305|0.106612325|  2.9437861|1.4627733|33.383865|8.3384846E-5|   0.70764375|\n",
      "|162246600|2023-06-12|         2|   0.01927032|0.035929356|0.034891304|  2.9437861|1.4627733|33.383865|8.3384846E-5| 0.0011660005|\n",
      "|265643382|2023-06-12|       366|   0.00447003| 0.04145695|0.048460145|        0.0|      0.0|33.383865|8.3384846E-5|   0.35420725|\n",
      "|276870487|2023-06-12|        77|   0.21603718| 0.04974834|0.067844205|  2.9437861|      0.0|33.383865|8.3384846E-5|    1.0552502|\n",
      "|290262168|2023-06-12|         2|   0.01713155|0.077386305|0.067844205|  2.9437861|1.4627733|33.383865|8.3384846E-5|   0.00203556|\n",
      "+---------+----------+----------+-------------+-----------+-----------+-----------+---------+---------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+-------------+-----------+-----------+-----------+---------+---------+------------+------------+\n",
      "|   job_id|      date|prediction|position_code| low_salary|high_salary|degree_code| exp_year| workdays|   area_code|   name_code|\n",
      "+---------+----------+----------+-------------+-----------+-----------+-----------+---------+---------+------------+------------+\n",
      "|286718149|2023-06-27|       329|      1.71354| 0.04145695|0.034891304|  3.5325434|      0.0|33.383865|8.3384846E-5|   0.8295007|\n",
      "|283904464|2023-06-27|       108|  0.012854009| 0.03316556|0.029076088|  2.9437861|      0.0|33.383865|  0.41951752|5.9288162E-5|\n",
      "|242540699|2023-06-27|         5|  0.006630188| 0.01658278|0.019384058|        0.0|      0.0|33.383865|8.3384846E-5|  0.06079013|\n",
      "|276671604|2023-06-27|       177|    4.2905016| 0.01658278|0.013568841|  2.3550289|1.4627733|33.383865|8.3384846E-5| 0.041284323|\n",
      "|262810834|2023-06-27|        71|   0.01927032|0.027637966|0.021322465|  2.3550289|1.4627733|33.383865|8.3384846E-5|0.0036561033|\n",
      "|251583368|2023-06-27|       238|  0.015056943| 0.44220746|  0.5039855|  2.9437861|4.8759108|33.383865|8.3384846E-5|  0.65653735|\n",
      "|251443500|2023-06-27|       114|  0.010929116| 0.00829139|0.007753623|  2.3550289|      0.0|33.383865|   4.5892687| 0.007648173|\n",
      "|292798545|2023-06-27|        26|    1.0716094|0.022110373|0.023260871|  2.9437861|      0.0|33.383865|8.3384846E-5| 0.019802246|\n",
      "|222772348|2023-06-27|       473|  0.002288484| 0.02487417|0.027137682|  2.3550289|      0.0|33.383865|8.3384846E-5|   0.2711445|\n",
      "|284432846|2023-06-27|        37| 0.0064376984| 0.03316556|0.038768116|  2.3550289|      0.0|33.383865|  0.08355995|   4.2262974|\n",
      "| 39553483|2023-06-27|       470|   0.43002114| 0.01658278|0.019384058|  2.3550289|1.4627733|33.383865|8.3384846E-5| 0.009683733|\n",
      "|280658857|2023-06-27|       320| 0.0023312594|0.055275932| 0.07753623|  2.9437861|1.4627733|33.383865|8.3384846E-5|  0.21934643|\n",
      "|287733719|2023-06-27|        13|    4.2903943|  19.346577|  15.507247|        0.0|      0.0|33.383865|8.3384846E-5|   1.4967296|\n",
      "|287126702|2023-06-27|         0| 0.0044914177| 0.04974834|0.048460145|  2.9437861|      0.0|33.383865|8.3384846E-5|  0.06831972|\n",
      "|289973777|2023-06-27|        56|  0.025686631|0.055275932|0.058152176|  2.9437861|1.4627733|33.383865|8.3384846E-5|   1.1969291|\n",
      "|286788627|2023-06-27|       386|    1.0738765|0.013818983|0.015507246|  2.3550289|      0.0|33.383865|8.3384846E-5|   0.3287726|\n",
      "|290936028|2023-06-27|        53|    0.6437912|0.027637966|0.029076088|  2.9437861|0.4875911|33.383865|8.3384846E-5|     2.19216|\n",
      "|291533779|2023-06-27|         1|  0.014992779| 0.22110373| 0.21322465|  2.9437861|4.8759108|33.383865|8.3384846E-5|  0.21786423|\n",
      "|260150056|2023-06-27|        23|   0.86196715|0.055275932|0.058152176|  2.9437861|1.4627733|33.383865|8.3384846E-5|   4.5202875|\n",
      "|272148580|2023-06-27|       249|    4.0701437| 0.06909492|0.058152176|        0.0|      0.0|33.383865|8.3384846E-5|  0.30199412|\n",
      "+---------+----------+----------+-------------+-----------+-----------+-----------+---------+---------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import datetime \n",
    "\n",
    "# 模拟命令行参数\n",
    "sys.argv = [\n",
    "     \"JobRec/sparksteps/1.py\",  # 脚本名称\n",
    "    \"2023-06-03\",  # start_date\n",
    "    \"2023-06-30\",  # end_date\n",
    "    \"/root/target_dir/JobRec-main\",  # path\n",
    "    \"bj\"  # city_short\n",
    "]\n",
    "\n",
    "# 以下是 1.py 脚本的其余代码\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"xxx\"\n",
    "\n",
    "K_job = 500\n",
    "K_person = 1000  # number of person classes ### same as the K in 16.data_xxx.ipynb\n",
    "numTopics = 1000  \n",
    "test_days = 5\n",
    "POS_CODE_LOW = '100000'\n",
    "POS_CODE_HIGH = '110000'\n",
    "start_date = sys.argv[1]\n",
    "end_date = sys.argv[2]\n",
    "path = sys.argv[3]\n",
    "file_path = 'file:///' + path\n",
    "city_short = sys.argv[4]\n",
    "\n",
    "def generate_train_test_dates(start_date, end_date):\n",
    "    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    date_list_train = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d')\n",
    "                 for x in range(0, (end - start).days + 1 - test_days)]\n",
    "    date_list_test = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d')\n",
    "                 for x in range((end - start).days + 1 - test_days, (end - start).days + 1)]\n",
    "    \n",
    "    return date_list_train, date_list_test\n",
    "\n",
    "dates_train, dates_test = generate_train_test_dates(start_date, end_date)\n",
    "print('dates tain:', dates_train)\n",
    "print('dates test:', dates_test)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"spark_data_query\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",500)\\\n",
    "    .config(\"spark.driver.memory\", \"16g\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "occ = spark.read.parquet(f'{file_path}/sparksteps/0data/202code2_query_{city_short}/occupation')\n",
    "occ_train = occ.filter(F.col('date').isin(dates_train)).cache()\n",
    "occ_test = occ.filter(F.col('date').isin(dates_test)).cache()\n",
    "occ.show(1)\n",
    "occ.printSchema()\n",
    "print('occ_train cnt:', occ_train.count())\n",
    "print('occ_test cnt:', occ_test.count())\n",
    "\n",
    "# 查看job postion 数量\n",
    "# job_pos = occ.select(F.col('position_lv3_code')).distinct()\n",
    "job_pos = occ.filter((F.col('position_lv3_code') >= POS_CODE_LOW) & (F.col('position_lv3_code') < POS_CODE_HIGH))\\\n",
    "                .select(F.col('position_lv3_code')).distinct().collect()\n",
    "job_pos_list = list(set([row['position_lv3_code'] for row in job_pos]))\n",
    "print('job pos cnt:', len(job_pos_list))\n",
    "print(job_pos_list)\n",
    "\n",
    "print('job 数值化1')\n",
    "# For training data\n",
    "# occ_train = occ_train.filter(F.col('position_lv3_code').isin(job_pos_list)).select(\\\n",
    "occ_train = occ_train.select(\\\n",
    "                 F.col('job_id').cast(LongType()),\\\n",
    "                 F.col('city_code').alias('city_code_str'), \\\n",
    "                 F.col('city_name'),\\\n",
    "                 F.col('update_time').cast(LongType()),\\\n",
    "                 F.col('position_lv3_code').alias('position_code').cast(IntegerType()),\\\n",
    "                 F.col('low_salary').cast(IntegerType()),\\\n",
    "                 F.col('high_salary').cast(IntegerType()),\\\n",
    "                 F.col('degree_code').cast(IntegerType()),\\\n",
    "                 F.col('experience_code').cast(IntegerType()),\\\n",
    "                 F.col('title').alias('name'),\\\n",
    "                 F.col('days_per_week_u').cast(IntegerType()),\\\n",
    "                 F.col('area_city_code_u').cast(IntegerType()),\\\n",
    "                 F.to_date(F.col('date')).alias('date'),\\\n",
    "                )\n",
    "print('\\tNULL field process...')\n",
    "occ_train = occ_train.fillna({'days_per_week_u': 5}).fillna(0).fillna(\"无\")\n",
    "occ_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "# occ_test = occ_test.filter(F.col('position_lv3_code').isin(job_pos_list)).select(\\\n",
    "occ_test = occ_test.select(\\\n",
    "                 F.col('job_id').cast(LongType()),\\\n",
    "                 F.col('city_code').alias('city_code_str'), \\\n",
    "                 F.col('city_name'),\\\n",
    "                 F.col('update_time').cast(LongType()),\\\n",
    "                 F.col('position_lv3_code').alias('position_code').cast(IntegerType()),\\\n",
    "                 F.col('low_salary').cast(IntegerType()),\\\n",
    "                 F.col('high_salary').cast(IntegerType()),\\\n",
    "                 F.col('degree_code').cast(IntegerType()),\\\n",
    "                 F.col('experience_code').cast(IntegerType()),\\\n",
    "                 F.col('title').alias('name'),\\\n",
    "                 F.col('days_per_week_u').cast(IntegerType()),\\\n",
    "                 F.col('area_city_code_u').cast(IntegerType()),\\\n",
    "                 F.to_date(F.col('date')).alias('date'),\\\n",
    "                )\n",
    "occ_test = occ_test.fillna({'days_per_week_u': 5}).fillna(0).fillna(\"无\")\n",
    "occ_test.show(1)\n",
    "\n",
    "print('job 数值化2')\n",
    "from pyspark.ml.feature import StringIndexer, StringIndexerModel\n",
    "\n",
    "inputs1 = [\"name\", \"city_code_str\"]\n",
    "outputs1 = [\"name_code\", \"city_code\"]\n",
    "\n",
    "# For training data\n",
    "stringIndexer1 = StringIndexer(inputCols=inputs1,outputCols=outputs1).setHandleInvalid('keep')\n",
    "model1 = stringIndexer1.fit(occ_train)\n",
    "model1.write().overwrite().save(f'{file_path}/sparksteps/models/StringIndexer_occ')\n",
    "\n",
    "occ_train = model1.transform(occ_train)\n",
    "# for name in inputs1:\n",
    "#     occ_train = occ_train.drop(name)\n",
    "occ_train = occ_train.withColumn('area_city_code_u', F.col('city_code')*10000 + F.col('area_city_code_u'))\n",
    "occ_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "if os.path.exists(f'{file_path}/sparksteps/models/StringIndexer_occ'):\n",
    "    model1 = StringIndexerModel.load(f'{file_path}/sparksteps/models/StringIndexer_occ')\n",
    "\n",
    "occ_test = model1.transform(occ_test)\n",
    "# for name in inputs1:\n",
    "#     occ_test = occ_test.drop(name)\n",
    "occ_test = occ_test.withColumn('area_city_code_u', F.col('city_code')*10000 + F.col('area_city_code_u'))\n",
    "occ_test.show(1)\n",
    "\n",
    "print('job 数值化3')\n",
    "def experience_change_to_year(num):\n",
    "    if num <= 100:\n",
    "        return 0\n",
    "    elif num == 101:\n",
    "        return 0\n",
    "    elif num == 103:\n",
    "        return 1\n",
    "    elif num == 104:\n",
    "        return 1.5\n",
    "    elif num == 105:\n",
    "        return 3\n",
    "    elif num == 106:\n",
    "        return 5.5\n",
    "    elif num == 107:\n",
    "        return 10\n",
    "    else:\n",
    "        return 0 # 应届生/在校生\n",
    "change_udf1 = F.udf(experience_change_to_year, IntegerType())\n",
    "\n",
    "def degree_change_to_level(num):\n",
    "    if num ==209:\n",
    "        return 1\n",
    "    elif num == 208:\n",
    "        return 2\n",
    "    elif num == 206:\n",
    "        return 3\n",
    "    elif num == 202:\n",
    "        return 4\n",
    "    elif num == 203:\n",
    "        return 5\n",
    "    elif num == 204:\n",
    "        return 6\n",
    "    elif num == 205:\n",
    "        return 7\n",
    "    else:\n",
    "        return 0\n",
    "change_udf2 = F.udf(degree_change_to_level, IntegerType())\n",
    "\n",
    "# For training data\n",
    "occ_train = occ_train.withColumn(\"experience_code\", change_udf1(occ_train.experience_code))\\\n",
    "            .withColumn(\"degree_code\", change_udf2(occ_train.degree_code))\\\n",
    "            .drop('city_code')\n",
    "occ_train.show(1)\n",
    "occ_train.write.mode('overwrite').parquet(f'{file_path}/sparksteps/train/occ_tmp')\n",
    "occ_train = occ_train.withColumn(\"position_code\", occ_train.position_code-100000)\n",
    "for name in inputs1:\n",
    "    occ_train = occ_train.drop(name)\n",
    "\n",
    "# For testing data\n",
    "occ_test = occ_test.withColumn(\"experience_code\", change_udf1(occ_test.experience_code))\\\n",
    "            .withColumn(\"degree_code\", change_udf2(occ_test.degree_code))\\\n",
    "            .drop('city_code')\n",
    "occ_test.show(1)\n",
    "occ_test.write.mode('overwrite').parquet(f'{file_path}/sparksteps/test/occ_tmp')\n",
    "occ_test = occ_test.withColumn(\"position_code\", occ_test.position_code-100000)\n",
    "for name in inputs1:\n",
    "    occ_test = occ_test.drop(name)\n",
    "    \n",
    "occ_train.show(1)\n",
    "occ_train = occ_train.dropna(how='any')\n",
    "occ_test.show(1)\n",
    "occ_test = occ_test.dropna(how='any')\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=['position_code', 'low_salary', 'high_salary', 'degree_code', \\\n",
    "                                             'experience_code', 'days_per_week_u', 'area_city_code_u', 'name_code'], \\\n",
    "                                  outputCol=\"features\")\n",
    "# tansdata = vectorAssembler.transform(occ).select('job_id', 'features', 'date')\n",
    "# tansdata.show(1)\n",
    "\n",
    "# For training data\n",
    "tansdata_train = vectorAssembler.transform(occ_train).select('job_id', 'features', 'date')\n",
    "tansdata_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "tansdata_test = vectorAssembler.transform(occ_test).select('job_id', 'features', 'date')\n",
    "tansdata_test.show(1)\n",
    "\n",
    "\n",
    "\n",
    "print('数据归一化')\n",
    "from pyspark.ml.feature import StandardScaler, StandardScalerModel\n",
    "\n",
    "standardScaler = StandardScaler(inputCol=\"features\",outputCol=\"scaledFeatures\")\n",
    "print(1)\n",
    "# For training data\n",
    "model_one = standardScaler.fit(tansdata_train)\n",
    "model_one.write().overwrite().save(f'{file_path}/sparksteps/models/StandardScaler_occ')\n",
    "\n",
    "print(2)\n",
    "stddata_train = model_one.transform(tansdata_train)\n",
    "stddata_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "if os.path.exists(f'{path}/hanxiao/models/StandardScaler_occ'):\n",
    "    model_one = StandardScalerModel.load(f'{file_path}/sparksteps/models/StandardScaler_occ')\n",
    "\n",
    "stddata_test = model_one.transform(tansdata_test)\n",
    "stddata_test.show(1)\n",
    "\n",
    "\n",
    "\n",
    "# import sklearn.cluster as cluster\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "\n",
    "# # traindata,testdata = stddata.randomSplit([0.8,0.2])\n",
    "# traindata = stddata.filter(F.col('date').isin(dates_train))\n",
    "# testdata = stddata.filter(F.col('date').isin(dates_test))\n",
    "# print(traindata.count(),testdata.count())\n",
    "\n",
    "print('KMeans')\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\",k=K_job,seed=1)\n",
    "\n",
    "# For training data\n",
    "model_two = kmeans.fit(stddata_train) # traindata<->stddata\n",
    "model_two.write().overwrite().save(f'{file_path}/sparksteps/models/KMeans_occ')\n",
    "predictions_train = model_two.transform(stddata_train) # testdata<->stddata\n",
    "\n",
    "# For testing data\n",
    "if os.path.exists(f'{file_path}/sparksteps/models/KMeans_occ'):\n",
    "    model_two = KMeansModel.load(f'{file_path}/sparksteps/models/KMeans_occ')\n",
    "\n",
    "predictions_test = model_two.transform(stddata_test) # testdata<->stddata\n",
    "\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "if os.path.exists(f'{file_path}/sparksteps/models/KMeans_occ'):\n",
    "    model_two = KMeansModel.load(f'{file_path}/sparksteps/models/KMeans_occ')\n",
    "    \n",
    "# # Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions_test)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "print('Cluster Center')\n",
    "print(model_two.clusterCenters()[0])\n",
    "clustercenters = model_two.clusterCenters()\n",
    "print(len(clustercenters))\n",
    "print(type(clustercenters))\n",
    "# print(clustercenters)\n",
    "\n",
    "import pickle as pkl\n",
    "with open(f'{path}/sparksteps//models/Clustercenters_occ.pkl', 'wb') as f:\n",
    "    pkl.dump(clustercenters, f)\n",
    "    \n",
    "def extract_feature(vec, index):\n",
    "    try:\n",
    "        return float(vec[index])\n",
    "    except IndexError:\n",
    "        return None\n",
    "split_udf = F.udf(extract_feature, FloatType())\n",
    "\n",
    "# For training data\n",
    "occ_pred_train = predictions_train.select(F.col('job_id'),\\\n",
    "                                       F.col('date'),\\\n",
    "                                       F.col('prediction'),\\\n",
    "                                       split_udf(predictions_train.scaledFeatures, F.lit(0)).alias('position_code'),\\\n",
    "                                       split_udf(predictions_train.scaledFeatures, F.lit(1)).alias('low_salary'),\\\n",
    "                                       split_udf(predictions_train.scaledFeatures, F.lit(2)).alias('high_salary'),\\\n",
    "                                       split_udf(predictions_train.scaledFeatures, F.lit(3)).alias('degree_code'),\\\n",
    "                                       split_udf(predictions_train.scaledFeatures, F.lit(4)).alias('exp_year'),\\\n",
    "                                       split_udf(predictions_train.scaledFeatures, F.lit(5)).alias('workdays'),\\\n",
    "                                       split_udf(predictions_train.scaledFeatures, F.lit(6)).alias('area_code'),\\\n",
    "                                       split_udf(predictions_train.scaledFeatures, F.lit(7)).alias('name_code'))\n",
    "occ_pred_train.show(20)\n",
    "\n",
    "# For testing data\n",
    "occ_pred_test = predictions_test.select(F.col('job_id'),\\\n",
    "                                       F.col('date'),\\\n",
    "                                       F.col('prediction'),\\\n",
    "                                       split_udf(predictions_test.scaledFeatures, F.lit(0)).alias('position_code'),\\\n",
    "                                       split_udf(predictions_test.scaledFeatures, F.lit(1)).alias('low_salary'),\\\n",
    "                                       split_udf(predictions_test.scaledFeatures, F.lit(2)).alias('high_salary'),\\\n",
    "                                       split_udf(predictions_test.scaledFeatures, F.lit(3)).alias('degree_code'),\\\n",
    "                                       split_udf(predictions_test.scaledFeatures, F.lit(4)).alias('exp_year'),\\\n",
    "                                       split_udf(predictions_test.scaledFeatures, F.lit(5)).alias('workdays'),\\\n",
    "                                       split_udf(predictions_test.scaledFeatures, F.lit(6)).alias('area_code'),\\\n",
    "                                       split_udf(predictions_test.scaledFeatures, F.lit(7)).alias('name_code'))\n",
    "occ_pred_test.show(20)\n",
    "\n",
    "# For training data\n",
    "occ_pred_train.write.mode('overwrite').parquet(f'{file_path}/sparksteps/train/occ')\n",
    "\n",
    "# For testing data\n",
    "occ_pred_test.write.mode('overwrite').parquet(f'{file_path}/sparksteps/test/occ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b4e516-f56f-4b16-baf4-fa520cc517c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates tain: ['2023-06-03', '2023-06-04', '2023-06-05', '2023-06-06', '2023-06-07', '2023-06-08', '2023-06-09', '2023-06-10', '2023-06-11', '2023-06-12', '2023-06-13', '2023-06-14', '2023-06-15', '2023-06-16', '2023-06-17', '2023-06-18', '2023-06-19', '2023-06-20', '2023-06-21', '2023-06-22', '2023-06-23', '2023-06-24', '2023-06-25']\n",
      "dates test: ['2023-06-26', '2023-06-27', '2023-06-28', '2023-06-29', '2023-06-30']\n",
      "+---------+----+------+-----------+----------+--------------+------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-----------------+----------------+------------------------+---------------+-------------+-----------------+----------------+------------------------+---------------+-------------+-----------------+----------------+------------------------+---------------+-------------+-----------------------+-----------------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "|  geek_id| age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect1_city_name|expect2_update_time|expect2_city_name|expect3_update_time|expect3_city_name|edu1_education_id|edu1_degree_code|edu1_standard_major_name|edu1_start_date|edu1_end_date|edu2_education_id|edu2_degree_code|edu2_standard_major_name|edu2_start_date|edu2_end_date|edu3_education_id|edu3_degree_code|edu3_standard_major_name|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_position_lv3_name|work1_industry_lv1_code|work1_industry_lv1_name|                         work1_skills|work1_start_date|work1_end_date|work2_position_lv3_code|work2_position_lv3_name|work2_industry_lv1_code|work2_industry_lv1_name|          work2_skills|work2_start_date|work2_end_date|work3_position_lv3_code|work3_position_lv3_name|work3_industry_lv1_code|work3_industry_lv1_name|      work3_skills|work3_start_date|work3_end_date|      date|\n",
      "+---------+----+------+-----------+----------+--------------+------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-----------------+----------------+------------------------+---------------+-------------+-----------------+----------------+------------------------+---------------+-------------+-----------------+----------------+------------------------+---------------+-------------+-----------------------+-----------------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "|502467001|41.0|     1|        204|        20|             0|           0|         1664945989|             北京|         1664945979|             北京|         1662537207|             北京|        117209061|             204|        计算机科学与技术|     1388505600|   1451577600|        117206220|             203|      电子信息科学与技术|      915120000|   1041350400|             NULL|            NULL|                    NULL|           NULL|         NULL|                 100701|               技术经理|                 100000|    互联网/IT/电子/通信|技术决策、系统架构、移动开发、研发...|      1283270400|    1656604800|                 100101|                   Java|                 100000|    互联网/IT/电子/通信|OSGi、Spring、MySQL...|      1151683200|    1283270400|                 100701|               技术经理|                 100000|    互联网/IT/电子/通信|技术管理、后端开发|      1056988800|    1151683200|2023-06-06|\n",
      "+---------+----+------+-----------+----------+--------------+------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-----------------+----------------+------------------------+---------------+-------------+-----------------+----------------+------------------------+---------------+-------------+-----------------+----------------+------------------------+---------------+-------------+-----------------------+-----------------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- geek_id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- degree_code: string (nullable = true)\n",
      " |-- work_years: string (nullable = true)\n",
      " |-- fresh_graduate: string (nullable = true)\n",
      " |-- apply_status: string (nullable = true)\n",
      " |-- expect1_update_time: string (nullable = true)\n",
      " |-- expect1_city_name: string (nullable = true)\n",
      " |-- expect2_update_time: string (nullable = true)\n",
      " |-- expect2_city_name: string (nullable = true)\n",
      " |-- expect3_update_time: string (nullable = true)\n",
      " |-- expect3_city_name: string (nullable = true)\n",
      " |-- edu1_education_id: string (nullable = true)\n",
      " |-- edu1_degree_code: string (nullable = true)\n",
      " |-- edu1_standard_major_name: string (nullable = true)\n",
      " |-- edu1_start_date: string (nullable = true)\n",
      " |-- edu1_end_date: string (nullable = true)\n",
      " |-- edu2_education_id: string (nullable = true)\n",
      " |-- edu2_degree_code: string (nullable = true)\n",
      " |-- edu2_standard_major_name: string (nullable = true)\n",
      " |-- edu2_start_date: string (nullable = true)\n",
      " |-- edu2_end_date: string (nullable = true)\n",
      " |-- edu3_education_id: string (nullable = true)\n",
      " |-- edu3_degree_code: string (nullable = true)\n",
      " |-- edu3_standard_major_name: string (nullable = true)\n",
      " |-- edu3_start_date: string (nullable = true)\n",
      " |-- edu3_end_date: string (nullable = true)\n",
      " |-- work1_position_lv3_code: string (nullable = true)\n",
      " |-- work1_position_lv3_name: string (nullable = true)\n",
      " |-- work1_industry_lv1_code: string (nullable = true)\n",
      " |-- work1_industry_lv1_name: string (nullable = true)\n",
      " |-- work1_skills: string (nullable = true)\n",
      " |-- work1_start_date: string (nullable = true)\n",
      " |-- work1_end_date: string (nullable = true)\n",
      " |-- work2_position_lv3_code: string (nullable = true)\n",
      " |-- work2_position_lv3_name: string (nullable = true)\n",
      " |-- work2_industry_lv1_code: string (nullable = true)\n",
      " |-- work2_industry_lv1_name: string (nullable = true)\n",
      " |-- work2_skills: string (nullable = true)\n",
      " |-- work2_start_date: string (nullable = true)\n",
      " |-- work2_end_date: string (nullable = true)\n",
      " |-- work3_position_lv3_code: string (nullable = true)\n",
      " |-- work3_position_lv3_name: string (nullable = true)\n",
      " |-- work3_industry_lv1_code: string (nullable = true)\n",
      " |-- work3_industry_lv1_name: string (nullable = true)\n",
      " |-- work3_skills: string (nullable = true)\n",
      " |-- work3_start_date: string (nullable = true)\n",
      " |-- work3_end_date: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person cnt: 44642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person_train cnt: 36133\n",
      "person_test cnt: 8509\n",
      "job 数值化1\n",
      "\tNULL field process...\n",
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+----------------+---------------+-------------+---------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "|  geek_id|age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time|  edu1_id|edu1_degree_code|      edu1_major|edu1_start_date|edu1_end_date|  edu2_id|edu2_degree_code|        edu2_major|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_major|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|                         work1_skills|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|          work2_skills|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|      work3_skills|work3_start_date|work3_end_date|      date|\n",
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+----------------+---------------+-------------+---------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "|502467001| 41|     1|        204|        20|             0|           0|         1664945989|         1664945979|         1662537207|117209061|             204|计算机科学与技术|     1388505600|   1451577600|117206220|             203|电子信息科学与技术|      915120000|   1041350400|      0|               0|        无|              0|            0|                 100701|                 100000|技术决策、系统架构、移动开发、研发...|      1283270400|    1656604800|                 100101|                 100000|OSGi、Spring、MySQL...|      1151683200|    1283270400|                 100701|                 100000|技术管理、后端开发|      1056988800|    1151683200|2023-06-06|\n",
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+----------------+---------------+-------------+---------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "\tNULL field process...\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+--------------------------+---------------+-------------+-------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+----------+\n",
      "| geek_id|age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time| edu1_id|edu1_degree_code|                edu1_major|edu1_start_date|edu1_end_date|edu2_id|edu2_degree_code|        edu2_major|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_major|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|            work1_skills|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|           work2_skills|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|                         work3_skills|work3_start_date|work3_end_date|      date|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+--------------------------+---------------+-------------+-------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+----------+\n",
      "|28317633| 28|     1|        203|         6|             0|           1|         1631762516|         1627016623|         1560231830|19969238|             203|电气工程及其自动化（函授）|     1483200000|   1546272000|5858924|             202|电力系统自动化技术|     1388505600|   1483200000|      0|               0|        无|              0|            0|                 100803|                 100900|传感器、系统集成、自动化|      1538323200|    1569859200|                 101402|                 100700|现场调试、电气安装、PLC|      1498838400|    1538323200|                 100601|                 100900|技术管理、项目管理、项目实施、资料...|      1569859200|             0|2023-06-29|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+--------------------------+---------------+-------------+-------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "job 数值化2\n",
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+----------------+---------------+-------------+---------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "|  geek_id|age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time|  edu1_id|edu1_degree_code|      edu1_major|edu1_start_date|edu1_end_date|  edu2_id|edu2_degree_code|        edu2_major|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_major|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|                         work1_skills|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|          work2_skills|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|      work3_skills|work3_start_date|work3_end_date|      date|\n",
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+----------------+---------------+-------------+---------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "|502467001| 41|     1|          6|        20|             0|           0|         1664945989|         1664945979|         1662537207|117209061|               6|计算机科学与技术|     1388505600|   1451577600|117206220|               5|电子信息科学与技术|      915120000|   1041350400|      0|               0|        无|              0|            0|                    701|                 100000|技术决策、系统架构、移动开发、研发...|      1283270400|    1656604800|                    101|                      0|OSGi、Spring、MySQL...|      1151683200|    1283270400|                    701|                 100000|技术管理、后端开发|      1056988800|    1151683200|2023-06-06|\n",
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+----------------+---------------+-------------+---------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+-----------------------+-----------------------+----------------------+----------------+--------------+-----------------------+-----------------------+------------------+----------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+--------------------------+---------------+-------------+-------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+----------+\n",
      "| geek_id|age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time| edu1_id|edu1_degree_code|                edu1_major|edu1_start_date|edu1_end_date|edu2_id|edu2_degree_code|        edu2_major|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_major|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|            work1_skills|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|           work2_skills|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|                         work3_skills|work3_start_date|work3_end_date|      date|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+--------------------------+---------------+-------------+-------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+----------+\n",
      "|28317633| 28|     1|          5|         6|             0|           1|         1631762516|         1627016623|         1560231830|19969238|               5|电气工程及其自动化（函授）|     1483200000|   1546272000|5858924|               4|电力系统自动化技术|     1388505600|   1483200000|      0|               0|        无|              0|            0|                    803|                 100900|传感器、系统集成、自动化|      1538323200|    1569859200|                   1402|                    700|现场调试、电气安装、PLC|      1498838400|    1538323200|                    601|                 100900|技术管理、项目管理、项目实施、资料...|      1569859200|             0|2023-06-29|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+--------------------------+---------------+-------------+-------+----------------+------------------+---------------+-------------+-------+----------------+----------+---------------+-------------+-----------------------+-----------------------+------------------------+----------------+--------------+-----------------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+-------------------------------------+----------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+---------------+-------------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------------------------------+\n",
      "|  geek_id|age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time|  edu1_id|edu1_degree_code|edu1_start_date|edu1_end_date|  edu2_id|edu2_degree_code|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|work3_start_date|work3_end_date|      date|                     text_combined|\n",
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+---------------+-------------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------------------------------+\n",
      "|502467001| 41|     1|          6|        20|             0|           0|         1664945989|         1664945979|         1662537207|117209061|               6|     1388505600|   1451577600|117206220|               5|      915120000|   1041350400|      0|               0|              0|            0|                    701|                 100000|      1283270400|    1656604800|                    101|                      0|      1151683200|    1283270400|                    701|                 100000|      1056988800|    1151683200|2023-06-06|[计算机科学与技术, 电子信息科学...|\n",
      "+---------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+----------------+---------------+-------------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------------------------------+\n",
      "| geek_id|age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time| edu1_id|edu1_degree_code|edu1_start_date|edu1_end_date|edu2_id|edu2_degree_code|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|work3_start_date|work3_end_date|      date|                     text_combined|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------------------------------+\n",
      "|28317633| 28|     1|          5|         6|             0|           1|         1631762516|         1627016623|         1560231830|19969238|               5|     1483200000|   1546272000|5858924|               4|     1388505600|   1483200000|      0|               0|              0|            0|                    803|                 100900|      1538323200|    1569859200|                   1402|                    700|      1498838400|    1538323200|                    601|                 100900|      1569859200|             0|2023-06-29|[电气工程及其自动化（函授）, 电...|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------------------------------+\n",
      "|  geek_id|      date|                     text_combined|\n",
      "+---------+----------+----------------------------------+\n",
      "|502467001|2023-06-06|[计算机科学与技术, 电子信息科学...|\n",
      "+---------+----------+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+----------+----------------------------------+\n",
      "| geek_id|      date|                     text_combined|\n",
      "+--------+----------+----------------------------------+\n",
      "|28317633|2023-06-29|[电气工程及其自动化（函授）, 电...|\n",
      "+--------+----------+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "\tCalculating Word Freq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLda traininig...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGet Topic\n",
      "\tUnderstand Topic\n",
      "\tGet topic per person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|geek_id1 |date1     |skills|\n",
      "+---------+----------+------+\n",
      "|502467001|2023-06-06|95    |\n",
      "+---------+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|geek_id1|date1     |skills|\n",
      "+--------+----------+------+\n",
      "|28317633|2023-06-29|183   |\n",
      "+--------+----------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "\tjoin operator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+------+\n",
      "| geek_id|age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time| edu1_id|edu1_degree_code|edu1_start_date|edu1_end_date| edu2_id|edu2_degree_code|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|work3_start_date|work3_end_date|      date|skills|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+------+\n",
      "|78153150| 38|     1|          6|        12|             0|           0|         1677154737|         1675390740|         1675390735|43895371|               6|     1230739200|   1293811200|44253209|               5|     1072886400|   1199116800|      0|               0|              0|            0|                    801|                 100900|      1635696000|    1677600000|                    801|                    900|      1588262400|    1619798400|                    801|                 100000|      1396281600|    1585670400|2023-06-15|   183|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "person cnt: 36133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+------+\n",
      "| geek_id|age|gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time| edu1_id|edu1_degree_code|edu1_start_date|edu1_end_date|edu2_id|edu2_degree_code|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|work3_start_date|work3_end_date|      date|skills|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+------+\n",
      "|68188947| 27|     1|          5|         3|             0|           2|         1638936448|         1618899511|         1618899459|36049931|               5|     1451577600|   1577808000|      0|               0|              0|            0|      0|               0|              0|            0|                    901|                 100200|      1593532800|    1627747200|                    901|                      0|      1561910400|    1572537600|                -100000|                  80000|               0|             0|2023-06-29|   833|\n",
      "+--------+---+------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+--------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "person cnt: 36133\n",
      "+--------+--------------------+----------+\n",
      "| geek_id|            features|      date|\n",
      "+--------+--------------------+----------+\n",
      "|78153150|[38.0,1.0,6.0,12....|2023-06-15|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+--------------------+----------+\n",
      "| geek_id|            features|      date|\n",
      "+--------+--------------------+----------+\n",
      "|68188947|[27.0,1.0,5.0,3.0...|2023-06-29|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "数据归一化\n",
      "+--------+--------------------+----------+--------------------+\n",
      "| geek_id|            features|      date|      scaledFeatures|\n",
      "+--------+--------------------+----------+--------------------+\n",
      "|78153150|[38.0,1.0,6.0,12....|2023-06-15|[6.89444056210421...|\n",
      "+--------+--------------------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+--------------------+----------+--------------------+\n",
      "| geek_id|            features|      date|      scaledFeatures|\n",
      "+--------+--------------------+----------+--------------------+\n",
      "|68188947|[27.0,1.0,5.0,3.0...|2023-06-29|[4.89868145202141...|\n",
      "+--------+--------------------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "KMeans\n",
      "Cluster Center\n",
      "1000\n",
      "<class 'list'>\n",
      "+--------+---------+---------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+---------+----------------+---------------+-------------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------+\n",
      "| geek_id|      age|   gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time|   skills|  edu1_id|edu1_degree_code|edu1_start_date|edu1_end_date|  edu2_id|edu2_degree_code|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|work3_start_date|work3_end_date|      date|prediction|\n",
      "+--------+---------+---------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+---------+----------------+---------------+-------------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------+\n",
      "|78153150|6.8944407|2.4487286|   9.619551| 2.4056966|           0.0|         0.0|          72.444626|          54.238125|           43.59566|0.5565488|0.7881603|        9.622245|      7.4769435|    7.8813496|0.9997925|       2.5154817|      1.9477624|    2.0134792|    0.0|             0.0|            0.0|          0.0|              2.6182425|              71.824104|       18.710108|      9.376527|               2.616299|              0.5152129|       14.098548|      2.626325|           0.0023528968|              10.341819|       1.9423177|     2.0895796|2023-06-15|       502|\n",
      "+--------+---------+---------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+---------+----------------+---------------+-------------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+---------+---------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------+\n",
      "| geek_id|      age|   gender|degree_code|work_years|fresh_graduate|apply_status|expect1_update_time|expect2_update_time|expect3_update_time|   skills|  edu1_id|edu1_degree_code|edu1_start_date|edu1_end_date|edu2_id|edu2_degree_code|edu2_start_date|edu2_end_date|edu3_id|edu3_degree_code|edu3_start_date|edu3_end_date|work1_position_lv3_code|work1_industry_lv1_code|work1_start_date|work1_end_date|work2_position_lv3_code|work2_industry_lv1_code|work2_start_date|work2_end_date|work3_position_lv3_code|work3_industry_lv1_code|work3_start_date|work3_end_date|      date|prediction|\n",
      "+--------+---------+---------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------+\n",
      "|68188947|4.8986816|2.4487286|   8.016293|0.60142416|           0.0|   1.6695402|          70.793785|          52.409313|           42.12569|2.5333614|0.6472921|       8.0185375|       8.818573|     9.611338|    0.0|             0.0|            0.0|          0.0|    0.0|             0.0|            0.0|          0.0|              2.9451141|               71.32582|       18.227818|      9.097887|              2.9429278|                    0.0|       13.864629|     2.5496967|            -0.29374492|               8.273455|             0.0|           0.0|2023-06-29|       485|\n",
      "+--------+---------+---------+-----------+----------+--------------+------------+-------------------+-------------------+-------------------+---------+---------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-------+----------------+---------------+-------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+-----------------------+-----------------------+----------------+--------------+----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person train cnt: 36133\n",
      "person test cnt: 8509\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import datetime \n",
    "import time\n",
    "\n",
    "# 模拟命令行参数\n",
    "sys.argv = [\n",
    "     \"JobRec/sparksteps/1.py\",  # 脚本名称\n",
    "    \"2023-06-03\",  # start_date\n",
    "    \"2023-06-30\",  # end_date\n",
    "    \"/root/target_dir/JobRec-main\",  # path\n",
    "    \"bj\"  # city_short\n",
    "]\n",
    "\n",
    "# 以下是 2.py 脚本的其余代码\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"xxx\"\n",
    "\n",
    "K_job = 500\n",
    "K_person = 1000  # number of person classes ### same as the K in 16.data_xxx.ipynb\n",
    "numTopics = 1000  \n",
    "test_days = 5\n",
    "POS_CODE_LOW = '100000'\n",
    "POS_CODE_HIGH = '110000'\n",
    "start_date = sys.argv[1]\n",
    "end_date = sys.argv[2]\n",
    "path = sys.argv[3]\n",
    "file_path = 'file:///' + path\n",
    "city_short = sys.argv[4]\n",
    "\n",
    "\n",
    "\n",
    "def generate_train_test_dates(start_date, end_date):\n",
    "    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    date_list_train = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range(0, (end - start).days + 1 - test_days)]\n",
    "    date_list_test = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range((end - start).days + 1 - test_days, (end - start).days + 1)]\n",
    "    \n",
    "    return date_list_train, date_list_test\n",
    "\n",
    "dates_train, dates_test = generate_train_test_dates(start_date, end_date)\n",
    "print('dates tain:', dates_train)\n",
    "print('dates test:', dates_test)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"spark_data_query\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",500)\\\n",
    "    .config(\"spark.driver.memory\", \"32g\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "person = spark.read.parquet(f'{file_path}/sparksteps/0data/202code2_query_{city_short}/person')\n",
    "person_train = person.filter(F.col('date').isin(dates_train)).cache()\n",
    "person_test = person.filter(F.col('date').isin(dates_test)).cache()\n",
    "person.show(1)\n",
    "person.printSchema()\n",
    "print('person cnt:', person.count())\n",
    "print('person_train cnt:', person_train.count())\n",
    "print('person_test cnt:', person_test.count())\n",
    "\n",
    "print('job 数值化1')\n",
    "\n",
    "# For training data\n",
    "person_train = person_train.select(\\\n",
    "                 F.col('geek_id').cast(LongType()),\\\n",
    "                 F.col('age').cast(IntegerType()), \\\n",
    "                 F.col('gender').cast(IntegerType()), \\\n",
    "                 F.col('degree_code').cast(IntegerType()),\\\n",
    "                 F.col('work_years').cast(IntegerType()),\\\n",
    "                 F.col('fresh_graduate').cast(IntegerType()),\\\n",
    "                 F.col('apply_status').cast(IntegerType()),\\\n",
    "                 F.col('expect1_update_time').cast(LongType()),\\\n",
    "                 F.col('expect2_update_time').cast(LongType()),\\\n",
    "                 F.col('expect3_update_time').cast(LongType()),\\\n",
    "                 F.col('edu1_education_id').alias('edu1_id').cast(IntegerType()),\\\n",
    "                 F.col('edu1_degree_code').cast(IntegerType()),\\\n",
    "                 F.col('edu1_standard_major_name').alias('edu1_major'),\\\n",
    "                 F.col('edu1_start_date').cast(LongType()),\\\n",
    "                 F.col('edu1_end_date').cast(LongType()),\\\n",
    "                 F.col('edu2_education_id').alias('edu2_id').cast(IntegerType()),\\\n",
    "                 F.col('edu2_degree_code').cast(IntegerType()),\\\n",
    "                 F.col('edu2_standard_major_name').alias('edu2_major'),\\\n",
    "                 F.col('edu2_start_date').cast(LongType()),\\\n",
    "                 F.col('edu2_end_date').cast(LongType()),\\\n",
    "                 F.col('edu3_education_id').alias('edu3_id').cast(IntegerType()),\\\n",
    "                 F.col('edu3_degree_code').cast(IntegerType()),\\\n",
    "                 F.col('edu3_standard_major_name').alias('edu3_major'),\\\n",
    "                 F.col('edu3_start_date').cast(LongType()),\\\n",
    "                 F.col('edu3_end_date').cast(LongType()),\\\n",
    "                 F.col('work1_position_lv3_code').cast(IntegerType()),\\\n",
    "                 F.col('work1_industry_lv1_code').cast(IntegerType()),\\\n",
    "                 F.col('work1_skills'),\\\n",
    "                 F.col('work1_start_date').cast(LongType()),\\\n",
    "                 F.col('work1_end_date').cast(LongType()),\\\n",
    "                 F.col('work2_position_lv3_code').cast(IntegerType()),\\\n",
    "                 F.col('work2_industry_lv1_code').cast(IntegerType()),\\\n",
    "                 F.col('work2_skills'),\\\n",
    "                 F.col('work2_start_date').cast(LongType()),\\\n",
    "                 F.col('work2_end_date').cast(LongType()),\\\n",
    "                 F.col('work3_position_lv3_code').cast(IntegerType()),\\\n",
    "                 F.col('work3_industry_lv1_code').cast(IntegerType()),\\\n",
    "                 F.col('work3_skills'),\\\n",
    "                 F.col('work3_start_date').cast(LongType()),\\\n",
    "                 F.col('work3_end_date').cast(LongType()),\\\n",
    "                 F.to_date(F.col('date')).alias('date'),\\\n",
    "                )\n",
    "print('\\tNULL field process...')\n",
    "person_train = person_train.fillna({'work1_industry_lv1_code': 80000,\\\n",
    "                       'work2_industry_lv1_code': 80000,\\\n",
    "                       'work3_industry_lv1_code': 80000,\\\n",
    "                       }).fillna(0).fillna(\"无\")\n",
    "person_train.show(1)\n",
    "\n",
    "\n",
    "\n",
    "# For testing data\n",
    "person_test = person_test.select(\\\n",
    "                 F.col('geek_id').cast(LongType()),\\\n",
    "                 F.col('age').cast(IntegerType()), \\\n",
    "                 F.col('gender').cast(IntegerType()), \\\n",
    "                 F.col('degree_code').cast(IntegerType()),\\\n",
    "                 F.col('work_years').cast(IntegerType()),\\\n",
    "                 F.col('fresh_graduate').cast(IntegerType()),\\\n",
    "                 F.col('apply_status').cast(IntegerType()),\\\n",
    "                 F.col('expect1_update_time').cast(LongType()),\\\n",
    "                 F.col('expect2_update_time').cast(LongType()),\\\n",
    "                 F.col('expect3_update_time').cast(LongType()),\\\n",
    "                 F.col('edu1_education_id').alias('edu1_id').cast(IntegerType()),\\\n",
    "                 F.col('edu1_degree_code').cast(IntegerType()),\\\n",
    "                 F.col('edu1_standard_major_name').alias('edu1_major'),\\\n",
    "                 F.col('edu1_start_date').cast(LongType()),\\\n",
    "                 F.col('edu1_end_date').cast(LongType()),\\\n",
    "                 F.col('edu2_education_id').alias('edu2_id').cast(IntegerType()),\\\n",
    "                 F.col('edu2_degree_code').cast(IntegerType()),\\\n",
    "                 F.col('edu2_standard_major_name').alias('edu2_major'),\\\n",
    "                 F.col('edu2_start_date').cast(LongType()),\\\n",
    "                 F.col('edu2_end_date').cast(LongType()),\\\n",
    "                 F.col('edu3_education_id').alias('edu3_id').cast(IntegerType()),\\\n",
    "                 F.col('edu3_degree_code').cast(IntegerType()),\\\n",
    "                 F.col('edu3_standard_major_name').alias('edu3_major'),\\\n",
    "                 F.col('edu3_start_date').cast(LongType()),\\\n",
    "                 F.col('edu3_end_date').cast(LongType()),\\\n",
    "                 F.col('work1_position_lv3_code').cast(IntegerType()),\\\n",
    "                 F.col('work1_industry_lv1_code').cast(IntegerType()),\\\n",
    "                 F.col('work1_skills'),\\\n",
    "                 F.col('work1_start_date').cast(LongType()),\\\n",
    "                 F.col('work1_end_date').cast(LongType()),\\\n",
    "                 F.col('work2_position_lv3_code').cast(IntegerType()),\\\n",
    "                 F.col('work2_industry_lv1_code').cast(IntegerType()),\\\n",
    "                 F.col('work2_skills'),\\\n",
    "                 F.col('work2_start_date').cast(LongType()),\\\n",
    "                 F.col('work2_end_date').cast(LongType()),\\\n",
    "                 F.col('work3_position_lv3_code').cast(IntegerType()),\\\n",
    "                 F.col('work3_industry_lv1_code').cast(IntegerType()),\\\n",
    "                 F.col('work3_skills'),\\\n",
    "                 F.col('work3_start_date').cast(LongType()),\\\n",
    "                 F.col('work3_end_date').cast(LongType()),\\\n",
    "                 F.to_date(F.col('date')).alias('date'),\\\n",
    "                )\n",
    "print('\\tNULL field process...')\n",
    "person_test = person_test.fillna({'work1_industry_lv1_code': 80000,\\\n",
    "                       'work2_industry_lv1_code': 80000,\\\n",
    "                       'work3_industry_lv1_code': 80000,\\\n",
    "                       }).fillna(0).fillna(\"无\")\n",
    "person_test.show(1)\n",
    "\n",
    "print('job 数值化2')\n",
    "def degree_change_to_level(num):\n",
    "    if num ==209:\n",
    "        return 1\n",
    "    elif num == 208:\n",
    "        return 2\n",
    "    elif num == 206:\n",
    "        return 3\n",
    "    elif num == 202:\n",
    "        return 4\n",
    "    elif num == 203:\n",
    "        return 5\n",
    "    elif num == 204:\n",
    "        return 6\n",
    "    elif num == 205:\n",
    "        return 7\n",
    "    else:\n",
    "        return 0\n",
    "change_udf = F.udf(degree_change_to_level, IntegerType())\n",
    "\n",
    "# For training data\n",
    "person_train = person_train.withColumn(\"degree_code\", change_udf(person_train.degree_code))\\\n",
    "            .withColumn(\"edu1_degree_code\", change_udf(person_train.edu1_degree_code))\\\n",
    "            .withColumn(\"edu2_degree_code\", change_udf(person_train.edu2_degree_code))\\\n",
    "            .withColumn(\"edu3_degree_code\", change_udf(person_train.edu3_degree_code))\\\n",
    "            .withColumn(\"work1_position_lv3_code\", person_train.work1_position_lv3_code-100000)\\\n",
    "            .withColumn(\"work2_position_lv3_code\", person_train.work2_position_lv3_code-100000)\\\n",
    "            .withColumn(\"work3_position_lv3_code\", person_train.work3_position_lv3_code-100000)\\\n",
    "            .withColumn(\"work2_industry_lv1_code\", person_train.work2_industry_lv1_code-100000)\n",
    "person_train.show(1)\n",
    "person_train.write.mode('overwrite').parquet(f'{file_path}/sparksteps/train/person_tmp')\n",
    "\n",
    "# For testing data\n",
    "person_test = person_test.withColumn(\"degree_code\", change_udf(person_test.degree_code))\\\n",
    "            .withColumn(\"edu1_degree_code\", change_udf(person_test.edu1_degree_code))\\\n",
    "            .withColumn(\"edu2_degree_code\", change_udf(person_test.edu2_degree_code))\\\n",
    "            .withColumn(\"edu3_degree_code\", change_udf(person_test.edu3_degree_code))\\\n",
    "            .withColumn(\"work1_position_lv3_code\", person_test.work1_position_lv3_code-100000)\\\n",
    "            .withColumn(\"work2_position_lv3_code\", person_test.work2_position_lv3_code-100000)\\\n",
    "            .withColumn(\"work3_position_lv3_code\", person_test.work3_position_lv3_code-100000)\\\n",
    "            .withColumn(\"work2_industry_lv1_code\", person_test.work2_industry_lv1_code-100000)\n",
    "person_test.show(1)\n",
    "person_test.write.mode('overwrite').parquet(f'{file_path}/sparksteps/test/person_tmp')\n",
    "\n",
    "\n",
    "def concat_arrays(array1, array2, array3, array4):\n",
    "    return array1 + array2 + array3 + array4\n",
    "\n",
    "\n",
    "concat_udf = F.udf(concat_arrays, ArrayType(StringType()))\n",
    "\n",
    "# For training data\n",
    "person_train = person_train.withColumn(\"text_combined\", concat_udf(F.array(\"edu1_major\", \"edu2_major\", \"edu3_major\"), \\\n",
    "                                          F.split(\"work1_skills\", \"、\"), F.split(\"work2_skills\", \"、\"), F.split(\"work3_skills\", \"、\")))\\\n",
    "        .drop('edu1_major', 'edu2_major', 'edu3_major', 'work1_skills', 'work2_skills', 'work3_skills')\n",
    "person_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "person_test = person_test.withColumn(\"text_combined\", concat_udf(F.array(\"edu1_major\", \"edu2_major\", \"edu3_major\"), \\\n",
    "                                          F.split(\"work1_skills\", \"、\"), F.split(\"work2_skills\", \"、\"), F.split(\"work3_skills\", \"、\")))\\\n",
    "        .drop('edu1_major', 'edu2_major', 'edu3_major', 'work1_skills', 'work2_skills', 'work3_skills')\n",
    "person_test.show(1)\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "from pyspark.ml.clustering import LDA, LocalLDAModel\n",
    "\n",
    "# For training data\n",
    "data_selected_train = person_train.select(\"geek_id\", \"date\", \"text_combined\").cache()\n",
    "data_selected_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "data_selected_test = person_test.select(\"geek_id\", \"date\", \"text_combined\").cache()\n",
    "data_selected_test.show(1)\n",
    "\n",
    "\n",
    "print('\\tCalculating Word Freq')\n",
    "vectorizer = CountVectorizer(inputCol=\"text_combined\",\n",
    "                             outputCol=\"features\")\n",
    "\n",
    "# For training data\n",
    "model = vectorizer.fit(data_selected_train)\n",
    "model.write().overwrite().save(f'{file_path}/sparksteps/models/CountVectorizer_person')\n",
    "\n",
    "featuresData_train = model.transform(data_selected_train)\n",
    "# featuresData.show(3)\n",
    "\n",
    "# For testing data\n",
    "if os.path.exists(f'{path}/models/CountVectorizer_person'):\n",
    "    model = CountVectorizerModel.load(f'{file_path}/sparksteps/models/CountVectorizer_person')\n",
    "    \n",
    "featuresData_test = model.transform(data_selected_test)\n",
    "# featuresData.show(3)\n",
    "\n",
    "print('\\tLda traininig...')\n",
    "# For training data\n",
    "lda = LDA(k=numTopics, maxIter=100)\n",
    "ldaModel = lda.fit(featuresData_train)\n",
    "ldaModel.write().overwrite().save(f'{file_path}/sparksteps/models/LDA_person')\n",
    "\n",
    "# For testing data\n",
    "if os.path.exists(f'{path}/models/LDA_person'):\n",
    "    ldaModel = LocalLDAModel.load(f'{file_path}/sparksteps/models/LDA_person')\n",
    "\n",
    "print('\\tGet Topic')\n",
    "# For training data\n",
    "transformedData_train = ldaModel.transform(featuresData_train)\n",
    "# transformedData.show()\n",
    "\n",
    "# For testing data\n",
    "transformedData_test = ldaModel.transform(featuresData_test)\n",
    "\n",
    "print('\\tUnderstand Topic')\n",
    "def getTopTopics(args):\n",
    "    geekId, date, topicDistribution = args\n",
    "    topTopics = topicDistribution.toArray().argsort()[-1:][::-1][0]\n",
    "    return (geekId, date, topTopics.tolist())\n",
    "\n",
    "print('\\tGet topic per person')\n",
    "# For training data\n",
    "documentTopics_train = transformedData_train.rdd.map(lambda row: (row[\"geek_id\"], row[\"date\"], row[\"topicDistribution\"])) \\\n",
    "    .map(getTopTopics) \\\n",
    "    .toDF([\"geek_id1\", \"date1\", \"skills\"])\n",
    "documentTopics_train.show(1, truncate=False)\n",
    "\n",
    "# For testing data\n",
    "documentTopics_test = transformedData_test.rdd.map(lambda row: (row[\"geek_id\"], row[\"date\"], row[\"topicDistribution\"])) \\\n",
    "    .map(getTopTopics) \\\n",
    "    .toDF([\"geek_id1\", \"date1\", \"skills\"])\n",
    "documentTopics_test.show(1, truncate=False)\n",
    "\n",
    "print('\\tjoin operator')\n",
    "# For training data\n",
    "person_train = person_train.join(documentTopics_train, (person_train.geek_id == documentTopics_train.geek_id1) & (person_train.date == documentTopics_train.date1), \\\n",
    "                     how=\"left\").drop(\"text_combined\", \"geek_id1\", \"date1\").cache()\n",
    "person_train.show(1)\n",
    "print('person cnt:', person_train.count())\n",
    "\n",
    "\n",
    "# For testing data\n",
    "person_test = person_test.join(documentTopics_test, (person_test.geek_id == documentTopics_test.geek_id1) & (person_test.date == documentTopics_test.date1), \\\n",
    "                     how=\"left\").drop(\"text_combined\", \"geek_id1\", \"date1\").cache()\n",
    "person_test.show(1)\n",
    "print('person cnt:', person_train.count())\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=['age','gender','degree_code','work_years','fresh_graduate','apply_status',\\\n",
    "                                             'expect1_update_time','expect2_update_time','expect3_update_time','skills',\\\n",
    "                                             'edu1_id','edu1_degree_code','edu1_start_date','edu1_end_date',\\\n",
    "                                             'edu2_id','edu2_degree_code','edu2_start_date','edu2_end_date',\\\n",
    "                                             'edu3_id','edu3_degree_code','edu3_start_date','edu3_end_date',\\\n",
    "                                             'work1_position_lv3_code','work1_industry_lv1_code','work1_start_date','work1_end_date',\\\n",
    "                                             'work2_position_lv3_code','work2_industry_lv1_code','work2_start_date','work2_end_date',\\\n",
    "                                             'work3_position_lv3_code','work3_industry_lv1_code','work3_start_date','work3_end_date'],\\\n",
    "                                  outputCol=\"features\")\n",
    "# For training data\n",
    "tansdata_train = vectorAssembler.transform(person_train).select('geek_id', 'features', 'date')\n",
    "tansdata_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "tansdata_test = vectorAssembler.transform(person_test).select('geek_id', 'features', 'date')\n",
    "tansdata_test.show(1)\n",
    "\n",
    "print('数据归一化')\n",
    "from pyspark.ml.feature import StandardScaler, StandardScalerModel\n",
    "\n",
    "# For training data\n",
    "standardScaler = StandardScaler(inputCol=\"features\",outputCol=\"scaledFeatures\")\n",
    "    \n",
    "model_one = standardScaler.fit(tansdata_train)\n",
    "model_one.write().overwrite().save(f'{file_path}/sparksteps/models/StandardScalerModel_person')\n",
    "\n",
    "stddata_train = model_one.transform(tansdata_train)\n",
    "stddata_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "if os.path.exists(f'{path}/models/StandardScalerModel_person'):\n",
    "    model_one = StandardScalerModel.load(f'{file_path}/sparksteps/models/StandardScalerModel_person')\n",
    "\n",
    "stddata_test = model_one.transform(tansdata_test)\n",
    "stddata_test.show(1)\n",
    "\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "\n",
    "# # traindata,testdata = stddata.randomSplit([0.8,0.2])\n",
    "# traindata = stddata.filter(F.col('date').isin(dates_train))\n",
    "# testdata = stddata.filter(F.col('date').isin(dates_test))\n",
    "# print(traindata.count(),testdata.count())\n",
    "\n",
    "print('KMeans')\n",
    "# For training data\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\",k=K_person,seed=1)\n",
    "\n",
    "model_two = kmeans.fit(stddata_train) # traindata<->stddata\n",
    "model_two.write().overwrite().save(f'{file_path}/sparksteps/models/KMeans_person')\n",
    "\n",
    "predictions_train = model_two.transform(stddata_train) # testdata<->stddata\n",
    "\n",
    "# For testing data\n",
    "if os.path.exists(f'{path}/models/KMeans_person'):\n",
    "    model_two = KMeansModel.load(f'{file_path}/sparksteps/models/KMeans_person')\n",
    "    \n",
    "predictions_test = model_two.transform(stddata_test) # testdata<->stddata\n",
    "\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "if os.path.exists(f'{path}/models/KMeans_person'):\n",
    "    model_two = KMeansModel.load(f'{file_path}/sparksteps/models/KMeans_person')\n",
    "\n",
    "# # Evaluate clustering by computing Silhouette score\n",
    "# evaluator = ClusteringEvaluator()\n",
    "\n",
    "# silhouette = evaluator.evaluate(predictions_test)\n",
    "# print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "print('Cluster Center')\n",
    "# print(model_two.clusterCenters())\n",
    "clustercenters = model_two.clusterCenters()\n",
    "print(len(clustercenters))\n",
    "print(type(clustercenters))\n",
    "\n",
    "import pickle as pkl\n",
    "with open(f'{path}/sparksteps/models/Clustercenters_person.pkl', 'wb') as f:\n",
    "    pkl.dump(clustercenters, f)\n",
    "    \n",
    "def extract_feature(vec, index):\n",
    "    try:\n",
    "        return float(vec[index])\n",
    "    except IndexError:\n",
    "        return None\n",
    "split_udf = F.udf(extract_feature, FloatType())\n",
    "\n",
    "# For training data\n",
    "person_pred_train = predictions_train.select(F.col('geek_id'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(0)).alias('age'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(1)).alias('gender'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(2)).alias('degree_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(3)).alias('work_years'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(4)).alias('fresh_graduate'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(5)).alias('apply_status'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(6)).alias('expect1_update_time'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(7)).alias('expect2_update_time'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(8)).alias('expect3_update_time'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(9)).alias('skills'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(10)).alias('edu1_id'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(11)).alias('edu1_degree_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(12)).alias('edu1_start_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(13)).alias('edu1_end_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(14)).alias('edu2_id'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(15)).alias('edu2_degree_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(16)).alias('edu2_start_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(17)).alias('edu2_end_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(18)).alias('edu3_id'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(19)).alias('edu3_degree_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(20)).alias('edu3_start_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(21)).alias('edu3_end_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(22)).alias('work1_position_lv3_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(23)).alias('work1_industry_lv1_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(24)).alias('work1_start_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(25)).alias('work1_end_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(26)).alias('work2_position_lv3_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(27)).alias('work2_industry_lv1_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(28)).alias('work2_start_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(29)).alias('work2_end_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(30)).alias('work3_position_lv3_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(31)).alias('work3_industry_lv1_code'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(32)).alias('work3_start_date'),\\\n",
    "                                          split_udf(predictions_train.scaledFeatures, F.lit(33)).alias('work3_end_date'),\\\n",
    "                                          F.col('date'),\n",
    "                                          F.col('prediction'))\n",
    "person_pred_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "person_pred_test = predictions_test.select(F.col('geek_id'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(0)).alias('age'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(1)).alias('gender'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(2)).alias('degree_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(3)).alias('work_years'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(4)).alias('fresh_graduate'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(5)).alias('apply_status'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(6)).alias('expect1_update_time'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(7)).alias('expect2_update_time'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(8)).alias('expect3_update_time'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(9)).alias('skills'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(10)).alias('edu1_id'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(11)).alias('edu1_degree_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(12)).alias('edu1_start_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(13)).alias('edu1_end_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(14)).alias('edu2_id'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(15)).alias('edu2_degree_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(16)).alias('edu2_start_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(17)).alias('edu2_end_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(18)).alias('edu3_id'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(19)).alias('edu3_degree_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(20)).alias('edu3_start_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(21)).alias('edu3_end_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(22)).alias('work1_position_lv3_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(23)).alias('work1_industry_lv1_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(24)).alias('work1_start_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(25)).alias('work1_end_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(26)).alias('work2_position_lv3_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(27)).alias('work2_industry_lv1_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(28)).alias('work2_start_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(29)).alias('work2_end_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(30)).alias('work3_position_lv3_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(31)).alias('work3_industry_lv1_code'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(32)).alias('work3_start_date'),\\\n",
    "                                          split_udf(predictions_test.scaledFeatures, F.lit(33)).alias('work3_end_date'),\\\n",
    "                                          F.col('date'),\n",
    "                                          F.col('prediction'))\n",
    "person_pred_test.show(1)\n",
    "\n",
    "person_pred_train.write.partitionBy(\"prediction\").mode('overwrite').parquet(f'{file_path}/sparksteps/train/person')\n",
    "person_pred_test.write.partitionBy(\"prediction\").mode('overwrite').parquet(f'{file_path}/sparksteps/test/person')\n",
    "\n",
    "print('person train cnt:', person_pred_train.count())\n",
    "print('person test cnt:', person_pred_test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d245f28-4559-496e-a8b3-e4d095773bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates tain: ['2023-06-03', '2023-06-04', '2023-06-05', '2023-06-06', '2023-06-07', '2023-06-08', '2023-06-09', '2023-06-10', '2023-06-11', '2023-06-12', '2023-06-13', '2023-06-14', '2023-06-15', '2023-06-16', '2023-06-17', '2023-06-18', '2023-06-19', '2023-06-20', '2023-06-21', '2023-06-22', '2023-06-23', '2023-06-24', '2023-06-25']\n",
      "dates test: ['2023-06-26', '2023-06-27', '2023-06-28', '2023-06-29', '2023-06-30']\n",
      "+---------+---------+---------+---------+----+----+----------+----------+----------+----------+\n",
      "|  geek_id|expect_id|   job_id|deal_type|page|rank| list_time| deal_time|        ds|      date|\n",
      "+---------+---------+---------+---------+----+----+----------+----------+----------+----------+\n",
      "|511625017|507990588|264144118|  no_list|   1|  63|1686550101|1686550101|2023-06-12|2023-06-12|\n",
      "| 20075741|  4114431|282889049|      det|   1|   1|1686536735|1686536739|2023-06-12|2023-06-12|\n",
      "|557749721|606045715|275914767|  no_list|   3|  31|1686551929|1686551929|2023-06-12|2023-06-12|\n",
      "|112902444|600710077|288227242|  no_list|   3| 360|1686553616|1686553616|2023-06-12|2023-06-12|\n",
      "| 42780374|601516547|283702029|  no_list|   4| 392|1686542309|1686542309|2023-06-12|2023-06-12|\n",
      "| 82183578|295714144|285531230|  no_list|   1| 203|1686526302|1686526302|2023-06-12|2023-06-12|\n",
      "|512103790|239950381|220869020|      det|   3|  -1|1686533075|1686533079|2023-06-12|2023-06-12|\n",
      "| 53964390|382594070|198236531|  no_list|   2|  41|1686534594|1686534594|2023-06-12|2023-06-12|\n",
      "| 28274075|367403518|197491330|  no_list|   6| 163|1686551396|1686551396|2023-06-12|2023-06-12|\n",
      "|510629386|240550260|203785788|  no_list|   1| 296|1686554753|1686554753|2023-06-12|2023-06-12|\n",
      "|558254916|603411753|289911340|  no_list|   3| 378|1686555749|1686555749|2023-06-12|2023-06-12|\n",
      "|554788956|489606510|282718863|  no_list|   1| 106|1686532683|1686532683|2023-06-12|2023-06-12|\n",
      "| 27167636|  7684540|289612276|  no_list|   2| 134|1686538097|1686538097|2023-06-12|2023-06-12|\n",
      "|570577013|482417457|288470607|  no_list|   1| 105|1686551715|1686551715|2023-06-12|2023-06-12|\n",
      "| 42707469|491352732|278772033|  no_list|   1| 335|1686565201|1686565201|2023-06-12|2023-06-12|\n",
      "| 96416872|246808287|173067566|  no_list|   6| 102|1686566767|1686566767|2023-06-12|2023-06-12|\n",
      "|503469710|595090884| 15506725|  no_list|   1| 349|1686578808|1686578808|2023-06-12|2023-06-12|\n",
      "| 30052814| 11696730| 99905097|  no_list|   2| 242|1686536806|1686536806|2023-06-12|2023-06-12|\n",
      "| 31708809|468447669|290164501|  no_list|   1|  32|1686556395|1686556395|2023-06-12|2023-06-12|\n",
      "|102608965|561029288|288434308|  no_list|   1| 193|1686537296|1686537296|2023-06-12|2023-06-12|\n",
      "+---------+---------+---------+---------+----+----+----------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- geek_id: string (nullable = true)\n",
      " |-- expect_id: string (nullable = true)\n",
      " |-- job_id: string (nullable = true)\n",
      " |-- deal_type: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- rank: string (nullable = true)\n",
      " |-- list_time: long (nullable = true)\n",
      " |-- deal_time: long (nullable = true)\n",
      " |-- ds: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation cnt: 3802340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_train cnt: 3038059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_test cnt: 764281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|  geekID|pred_person|date_person|\n",
      "+--------+-----------+-----------+\n",
      "|28011567|         32| 2023-06-14|\n",
      "+--------+-----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person train cnt: 36133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|  geekID|pred_person|date_person|\n",
      "+--------+-----------+-----------+\n",
      "|21380403|        521| 2023-06-26|\n",
      "+--------+-----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person test cnt: 8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp person train cnt: 36133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp person test cnt: 8509\n",
      "+---------+--------+----------+\n",
      "|    jobID|pred_job|  date_job|\n",
      "+---------+--------+----------+\n",
      "|288845965|     276|2023-06-12|\n",
      "+---------+--------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "occ train cnt: 889821\n",
      "+---------+--------+----------+\n",
      "|    jobID|pred_job|  date_job|\n",
      "+---------+--------+----------+\n",
      "|225592014|     169|2023-06-27|\n",
      "+---------+--------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "occ test cnt: 218797\n",
      "tmp occ train cnt: 889821\n",
      "tmp occ test cnt: 218797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "| geek_id|expect_id|   job_id|deal_type|page|rank| list_time| deal_time|        ds|      date|pred_person|pred_job|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "|81879375|557325420|273822681|  no_list|  17| 249|1686528662|1686528662|2023-06-12|2023-06-12|        867|     319|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log train cnt: 1478255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "| geek_id|expect_id|   job_id|deal_type|page|rank| list_time| deal_time|        ds|      date|pred_person|pred_job|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "|30107126| 11631994|293112247|      det|   3|  13|1687847802|1687847853|2023-06-27|2023-06-27|        722|       2|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log test cnt: 361874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "| geek_id|expect_id|   job_id|deal_type|page|rank| list_time| deal_time|        ds|      date|pred_person|pred_job|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "|34706274| 19223884|174569378|      det|   1|  12|1686721067|1686721136|2023-06-14|2023-06-14|          0|       0|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "| geek_id|expect_id|   job_id|deal_type|page|rank| list_time| deal_time|        ds|      date|pred_person|pred_job|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "|39472752|498715271|236013163|  no_list|   3| 156|1687808106|1687808106|2023-06-27|2023-06-27|          0|       0|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log train cnt: 1478255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log test cnt: 361874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log train cnt: 1474995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log test cnt: 361741\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import datetime \n",
    "import time\n",
    "\n",
    "# 模拟命令行参数\n",
    "sys.argv = [\n",
    "     \"JobRec/sparksteps/1.py\",  # 脚本名称\n",
    "    \"2023-06-03\",  # start_date\n",
    "    \"2023-06-30\",  # end_date\n",
    "    \"/root/target_dir/JobRec-main\",  # path\n",
    "    \"bj\"  # city_short\n",
    "]\n",
    "\n",
    "# 以下是 3.py 脚本的其余代码\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"xxx\"\n",
    "\n",
    "K_job = 500\n",
    "K_person = 1000  # number of person classes ### same as the K in 16.data_xxx.ipynb\n",
    "numTopics = 1000  \n",
    "test_days = 5\n",
    "POS_CODE_LOW = '100000'\n",
    "POS_CODE_HIGH = '110000'\n",
    "start_date = sys.argv[1]\n",
    "end_date = sys.argv[2]\n",
    "path = sys.argv[3]\n",
    "file_path = 'file:///' + path\n",
    "city_short = sys.argv[4]\n",
    "\n",
    "def generate_train_test_dates(start_date, end_date):\n",
    "    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    date_list_train = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range(0, (end - start).days + 1 - test_days)]\n",
    "    date_list_test = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range((end - start).days + 1 - test_days, (end - start).days + 1)]\n",
    "    \n",
    "    return date_list_train, date_list_test\n",
    "\n",
    "dates_train, dates_test = generate_train_test_dates(start_date, end_date)\n",
    "print('dates tain:', dates_train)\n",
    "print('dates test:', dates_test)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"spark_data_query\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",500)\\\n",
    "    .config(\"spark.driver.memory\", \"32g\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "log = spark.read.parquet(f'{file_path}/sparksteps/0data/202code2_query_{city_short}/log')\n",
    "log_train = log.filter(F.col('date').isin(dates_train)).cache()\n",
    "log_test = log.filter(F.col('date').isin(dates_test)).cache()\n",
    "log.show(20)\n",
    "log.printSchema()\n",
    "print('relation cnt:', log.count())\n",
    "print('log_train cnt:', log_train.count())\n",
    "print('log_test cnt:', log_test.count())\n",
    "\n",
    "# For training data\n",
    "person_train = spark.read.parquet(f'{file_path}/sparksteps/train/person')\\\n",
    "                        .select(F.col('geek_id').alias('geekID'),F.col('prediction').alias('pred_person'),F.col('date').alias('date_person'))\n",
    "person_train.show(1)\n",
    "print('person train cnt:', person_train.count())\n",
    "\n",
    "# For testing data\n",
    "person_test = spark.read.parquet(f'{file_path}/sparksteps/test/person')\\\n",
    "                        .select(F.col('geek_id').alias('geekID'),F.col('prediction').alias('pred_person'),F.col('date').alias('date_person'))\n",
    "person_test.show(1)\n",
    "print('person test cnt:', person_test.count())\n",
    "\n",
    "# For training data\n",
    "tmp_person_train = person_train.distinct()\n",
    "print('tmp person train cnt:', tmp_person_train.count())\n",
    "\n",
    "# For testing data\n",
    "tmp_person_test = person_test.distinct()\n",
    "print('tmp person test cnt:', tmp_person_test.count())\n",
    "\n",
    "# For training data\n",
    "occ_train = spark.read.parquet(f'{file_path}/sparksteps/train/occ')\\\n",
    "                    .select(F.col('job_id').alias('jobID'),F.col('prediction').alias('pred_job'),F.col('date').alias('date_job'))\n",
    "occ_train.show(1)\n",
    "print('occ train cnt:', occ_train.count())\n",
    "\n",
    "# For testing data\n",
    "occ_test = spark.read.parquet(f'{file_path}/sparksteps/test/occ')\\\n",
    "                    .select(F.col('job_id').alias('jobID'),F.col('prediction').alias('pred_job'),F.col('date').alias('date_job'))\n",
    "occ_test.show(1)\n",
    "print('occ test cnt:', occ_test.count())\n",
    "\n",
    "# For training data\n",
    "tmp_occ_train = occ_train.distinct()\n",
    "print('tmp occ train cnt:', tmp_occ_train.count())\n",
    "\n",
    "# For testing data\n",
    "tmp_occ_test = occ_test.distinct()\n",
    "print('tmp occ test cnt:', tmp_occ_test.count())\n",
    "\n",
    "# For training data\n",
    "log_train = log_train.join(person_train, (log_train.geek_id == person_train.geekID) & (log_train.ds == person_train.date_person), 'left')\\\n",
    "            .drop(\"geekID\", 'date_person').filter(F.col('pred_person').isNotNull())\n",
    "log_train = log_train.join(occ_train, (log_train.job_id == occ_train.jobID) & (log_train.ds == occ_train.date_job), 'left')\\\n",
    "            .drop('jobID', 'date_job').filter(F.col('pred_job').isNotNull())\n",
    "log_train.show(1)\n",
    "print('log train cnt:', log_train.count())\n",
    "\n",
    "# For testing data\n",
    "log_test = log_test.join(person_test, (log_test.geek_id == person_test.geekID) & (log_test.ds == person_test.date_person), 'left')\\\n",
    "            .drop(\"geekID\", 'date_person').filter(F.col('pred_person').isNotNull())\n",
    "log_test = log_test.join(occ_test, (log_test.job_id == occ_test.jobID) & (log_test.ds == occ_test.date_job), 'left')\\\n",
    "            .drop('jobID', 'date_job').filter(F.col('pred_job').isNotNull())\n",
    "log_test.show(1)\n",
    "print('log test cnt:', log_test.count())\n",
    "\n",
    "# For training data\n",
    "log_train = log_train.orderBy('pred_person', 'pred_job', 'geek_id', 'job_id', 'date')\n",
    "log_train.show(1)\n",
    "\n",
    "# For testing data\n",
    "log_test = log_test.orderBy('pred_person', 'pred_job', 'geek_id', 'job_id', 'date')\n",
    "log_test.show(1)\n",
    "\n",
    "# log.orderBy('geek_id', 'deal_time', 'pred_person', 'pred_job', 'job_id').show(10000)\n",
    "\n",
    "log_train.write.mode('overwrite').parquet(f'{file_path}/sparksteps/train/log')\n",
    "log_test.write.mode('overwrite').parquet(f'{file_path}/sparksteps/test/log')\n",
    "print('log train cnt:', log_train.count())\n",
    "print('log test cnt:', log_test.count())\n",
    "\n",
    "print('log train cnt:', log_train.distinct().count())\n",
    "print('log test cnt:', log_test.distinct().count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60408b0d-8db1-4759-9cb6-9d0f6664e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates tain: ['2023-06-03', '2023-06-04', '2023-06-05', '2023-06-06', '2023-06-07', '2023-06-08', '2023-06-09', '2023-06-10', '2023-06-11', '2023-06-12', '2023-06-13', '2023-06-14', '2023-06-15', '2023-06-16', '2023-06-17', '2023-06-18', '2023-06-19', '2023-06-20', '2023-06-21', '2023-06-22', '2023-06-23', '2023-06-24', '2023-06-25']\n",
      "dates test: ['2023-06-26', '2023-06-27', '2023-06-28', '2023-06-29', '2023-06-30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "| geek_id|expect_id|   job_id|deal_type|page|rank| list_time| deal_time|        ds|      date|pred_person|pred_job|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "|12542372|  2676929|284394150|  no_list|   1|  71|1686585317|1686585317|2023-06-12|2023-06-12|         54|       0|\n",
      "+--------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "log train cnt: 1478255\n",
      "+---------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "|  geek_id|expect_id|   job_id|deal_type|page|rank| list_time| deal_time|        ds|      date|pred_person|pred_job|\n",
      "+---------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "|123508450|609796781|206280492|  no_list|   1|  87|1687867035|1687867035|2023-06-27|2023-06-27|        922|       2|\n",
      "+---------+---------+---------+---------+----+----+----------+----------+----------+----------+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "log test cnt: 361874\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|  geek_id|         job_id_list|      deal_type_list|      deal_time_list|           date_list|    pred_person_list|       pred_job_list|list_size|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|100748452|[285573073, 28541...|[det, det, no_lis...|[1685933831, 1686...|[2023-06-05, 2023...|[145, 145, 145, 1...|[307, 421, 78, 13...|       14|\n",
      "|101287947|[289796108, 29108...|[no_list, no_list...|[1687229474, 1687...|[2023-06-20, 2023...|[58, 58, 58, 58, ...|[2, 35, 36, 36, 6...|      172|\n",
      "|102122964|[217319314, 23182...|[det, no_list, no...|[1686549362, 1686...|[2023-06-12, 2023...|[383, 383, 383, 3...|[295, 2, 2, 2, 2,...|       12|\n",
      "|103046815|[286848826, 28450...|     [det, det, det]|[1686102989, 1686...|[2023-06-07, 2023...|     [350, 350, 350]|        [5, 320, 71]|        3|\n",
      "|103046815|[181992004, 26937...|[no_list, no_list...|[1686793382, 1686...|[2023-06-15, 2023...|[350, 350, 350, 3...|[2, 2, 2, 2, 2, 5...|      412|\n",
      "|103894186|[291254208, 17481...|[det, det, det, d...|[1687277108, 1687...|[2023-06-21, 2023...|[623, 623, 623, 6...|[92, 318, 252, 32...|       46|\n",
      "|103924744|[288696597, 18580...|[no_list, no_list...|[1685782857, 1685...|[2023-06-03, 2023...|[76, 76, 76, 76, ...|[44, 79, 95, 116,...|      148|\n",
      "|103924744|         [290246531]|               [det]|        [1686555225]|        [2023-06-12]|                [76]|               [299]|        1|\n",
      "|104388858|[222753528, 21729...|[det, det, succes...|[1685928743, 1685...|[2023-06-05, 2023...|[693, 693, 693, 6...|[71, 71, 273, 2, ...|       34|\n",
      "|106113631|         [288673099]|               [det]|        [1686039140]|        [2023-06-06]|               [463]|               [319]|        1|\n",
      "|106113631|[271538395, 28067...|[det, det, addf, ...|[1686741343, 1686...|[2023-06-14, 2023...|[463, 463, 463, 4...|[320, 451, 367, 3...|       61|\n",
      "|106232247|[216324219, 28843...|[det, det, det, d...|[1685935395, 1685...|[2023-06-05, 2023...|[930, 930, 930, 9...|[451, 157, 104, 0...|      180|\n",
      "|106787870|[289380957, 28528...|[addf, addf, det,...|[1685949126, 1685...|[2023-06-05, 2023...|[457, 457, 457, 4...|[221, 2, 273, 2, ...|      220|\n",
      "|107528624|[182662345, 27216...|[no_list, no_list...|[1685847548, 1685...|[2023-06-04, 2023...|[582, 582, 582, 5...|[1, 187, 188, 188...|       10|\n",
      "|108163269|[288203761, 26951...|          [det, det]|[1686154292, 1686...|[2023-06-08, 2023...|          [164, 164]|           [448, 15]|        2|\n",
      "|108332561|[234037298, 29148...|[no_list, no_list...|[1686742665, 1686...|[2023-06-14, 2023...|[752, 752, 752, 7...|[0, 0, 1, 2, 2, 2...|       45|\n",
      "|109653007|[215799257, 25298...|[no_list, no_list...|[1685845928, 1685...|[2023-06-04, 2023...|[642, 642, 642, 6...|[71, 96, 126, 130...|       42|\n",
      "|109750663|[241860419, 28587...|[no_list, no_list...|[1686634514, 1686...|[2023-06-13, 2023...|[589, 589, 589, 5...|[0, 2, 107, 319, ...|       16|\n",
      "|109985420|[289974948, 29020...|[no_list, no_list...|[1686549089, 1686...|[2023-06-12, 2023...|[249, 249, 249, 2...|[12, 12, 63, 89, ...|       44|\n",
      "|110147910|[123554897, 25929...|[no_list, no_list...|[1685760707, 1685...|[2023-06-03, 2023...|[51, 51, 51, 51, ...|[0, 0, 0, 2, 2, 2...|     1302|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|  geek_id|         job_id_list|      deal_type_list|      deal_time_list|           date_list|    pred_person_list|       pred_job_list|list_size|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|100407364|[266632030, 26335...|[addf, addf, addf...|[1687746354, 1687...|[2023-06-26, 2023...|[820, 820, 820, 8...|[0, 87, 2, 87, 2,...|      368|\n",
      "|101344699|[256930115, 25903...|[no_list, no_list...|[1688008446, 1688...|[2023-06-29, 2023...|[548, 548, 548, 5...|[2, 2, 2, 2, 56, ...|       12|\n",
      "|103894186|[265112381, 26511...|[det, det, det, d...|[1687738999, 1687...|[2023-06-26, 2023...|[623, 623, 623, 6...|[54, 54, 250, 250...|      148|\n",
      "|104371617|[282006308, 26991...|[det, det, det, d...|[1687745034, 1687...|[2023-06-26, 2023...|[416, 416, 416, 4...|[0, 0, 0, 451, 77...|      509|\n",
      "|106113631|[271562690, 28862...|[addf, no_list, n...|[1687756658, 1688...|[2023-06-26, 2023...|[463, 463, 463, 4...|[37, 0, 35, 37, 8...|       18|\n",
      "|106232247|[290399464, 29373...|[no_list, no_list...|[1687923736, 1687...|[2023-06-28, 2023...|[930, 930, 930, 9...|[0, 0, 0, 2, 2, 5...|       22|\n",
      "|108158249|[240394528, 27289...|[det, det, det, det]|[1687745616, 1688...|[2023-06-26, 2023...|[538, 538, 538, 538]|  [381, 343, 37, 37]|        4|\n",
      "|108332561|[286878762, 28603...|[no_list, no_list...|[1687912049, 1687...|[2023-06-28, 2023...|[752, 752, 752, 7...|[0, 1, 2, 2, 2, 3...|       16|\n",
      "|108982013|[176150931, 12495...|[no_list, no_list...|[1687949934, 1687...|[2023-06-28, 2023...|[628, 628, 628, 6...|[0, 2, 2, 2, 2, 2...|       30|\n",
      "|110147910|[293353880, 29313...|[det, addf, succe...|[1687745108, 1687...|[2023-06-26, 2023...|[51, 51, 51, 51, ...|[2, 265, 2, 87, 2...|      442|\n",
      "|110297764|[174810332, 27289...|[no_list, no_list...|[1687836645, 1687...|[2023-06-27, 2023...|[885, 885, 885, 8...|[0, 0, 2, 2, 2, 2...|       80|\n",
      "|110572027|[286257328, 28851...|[no_list, no_list...|[1688054468, 1688...|[2023-06-30, 2023...|[538, 538, 538, 5...|[1, 1, 2, 2, 2, 2...|       11|\n",
      "|111412576|[290322939, 29126...|[det, det, det, d...|[1687736305, 1687...|[2023-06-26, 2023...|[760, 760, 760, 7...|[26, 116, 95, 119...|       43|\n",
      "|112454019|[176006484, 26592...|[det, no_list, no...|[1687831134, 1687...|[2023-06-27, 2023...|[571, 571, 571, 5...|[0, 0, 0, 252, 25...|       97|\n",
      "|   113012|[293331372, 24934...|[det, det, det, d...|[1687742706, 1687...|[2023-06-26, 2023...|[465, 465, 465, 4...|[71, 311, 309, 30...|      277|\n",
      "|113112989|[275141070, 28084...|[no_list, no_list...|[1688136371, 1688...|[2023-06-30, 2023...|[637, 637, 637, 6...|[5, 138, 238, 238...|       22|\n",
      "|113217027|         [244533819]|               [det]|        [1687736625]|        [2023-06-26]|               [135]|               [441]|        1|\n",
      "|113260752|[185987475, 45225...|[addf, det, succe...|[1687739937, 1687...|[2023-06-26, 2023...|[592, 592, 592, 5...|[289, 493, 65, 86...|      159|\n",
      "|113584330|[270280095, 27235...|[no_list, no_list...|[1687943961, 1687...|[2023-06-28, 2023...|[316, 316, 316, 3...|[0, 2, 2, 2, 2, 2...|       43|\n",
      "|114198188|[278710424, 29132...|[no_list, no_list...|[1687853248, 1687...|[2023-06-27, 2023...|[824, 824, 824, 8...|[2, 2, 26, 64, 12...|      233|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log train cnt: 10725\n",
      "log test cnt: 4109\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import datetime \n",
    "import json\n",
    "\n",
    "# 模拟命令行参数\n",
    "sys.argv = [\n",
    "     \"JobRec/sparksteps/1.py\",  # 脚本名称\n",
    "    \"2023-06-03\",  # start_date\n",
    "    \"2023-06-30\",  # end_date\n",
    "    \"/root/target_dir/JobRec-main\",  # path\n",
    "    \"bj\"  # city_short\n",
    "]\n",
    "\n",
    "# 以下是 4.py 脚本的其余代码\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"xxx\"\n",
    "\n",
    "\n",
    "ddays = 7\n",
    "\n",
    "\n",
    "K_job = 500\n",
    "K_person = 1000  # number of person classes ### same as the K in 16.data_xxx.ipynb\n",
    "numTopics = 1000  \n",
    "test_days = 5\n",
    "POS_CODE_LOW = '100000'\n",
    "POS_CODE_HIGH = '110000'\n",
    "start_date = sys.argv[1]\n",
    "end_date = sys.argv[2]\n",
    "path = sys.argv[3]\n",
    "file_path = 'file:///' + path\n",
    "city_short = sys.argv[4]\n",
    "\n",
    "def generate_train_test_dates(start_date, end_date):\n",
    "    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "    \n",
    "    date_list_train = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range(0, (end - start).days + 1 - test_days)]\n",
    "    date_list_test = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range((end - start).days + 1 - test_days, (end - start).days + 1)]\n",
    "    \n",
    "    return date_list_train, date_list_test\n",
    "\n",
    "dates_train, dates_test = generate_train_test_dates(start_date, end_date)\n",
    "print('dates tain:', dates_train)\n",
    "print('dates test:', dates_test)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"spark_data_query\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",500)\\\n",
    "    .config(\"spark.driver.memory\", \"32g\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# For training data\n",
    "log_train = spark.read.parquet(f'{file_path}/sparksteps/train/log').cache()\n",
    "log_train.show(1)\n",
    "# person.printSchema()\n",
    "print('log train cnt:', log_train.count())\n",
    "\n",
    "# For testing data\n",
    "log_test = spark.read.parquet(f'{file_path}/sparksteps//test/log').cache()\n",
    "log_test.show(1)\n",
    "# person.printSchema()\n",
    "print('log test cnt:', log_test.count())\n",
    "\n",
    "# For training data\n",
    "\n",
    "\n",
    "\n",
    "windowSpec = Window.partitionBy(\"geek_id\").orderBy(\"deal_time\")\n",
    "\n",
    "# log = log.withColumn(\"next_deal_time\", F.lead(\"deal_time\", 1).over(windowSpec))\n",
    "# log = log.withColumn(\"next_pred_person\", F.lead(\"pred_person\", 1).over(windowSpec))\n",
    "\n",
    "# time_diff = (F.col(\"next_deal_time\") - F.col(\"deal_time\")) / 86400\n",
    "# log = log.withColumn(\"time_diff\", time_diff)\n",
    "# log = log.filter((F.col(\"pred_person\") == F.col(\"next_pred_person\")) & (F.col(\"time_diff\") <= ddays))\n",
    "\n",
    "log_train = log_train.withColumn(\"prev_deal_time\", F.lag(\"deal_time\").over(windowSpec))\n",
    "log_train = log_train.withColumn(\"time_diff\", F.when(F.isnull(log_train.deal_time - log_train.prev_deal_time), 0)\n",
    "                                  .otherwise(log_train.deal_time - log_train.prev_deal_time))\n",
    "\n",
    "log_train = log_train.withColumn(\"new_record\", F.when(log_train.time_diff > ddays * 86400, 1).otherwise(0))\n",
    "\n",
    "log_train = log_train.withColumn(\"group_id\", F.sum(\"new_record\").over(windowSpec))\n",
    "\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# inputs1 = [\"job_id\"]\n",
    "# outputs1 = [\"job_code\"]\n",
    "# stringIndexer1 = StringIndexer(inputCols=inputs1,outputCols=outputs1).setHandleInvalid('keep')\n",
    "# model1 = stringIndexer1.fit(log_train)\n",
    "# model1.write().overwrite().save(f'{file_path}/sparksteps/models/StringIndexer_prepare')\n",
    "# log_train = model1.transform(log_train)\n",
    "\n",
    "# min_job_code = log.select(F.min(\"job_code\")).first()[0]\n",
    "# max_job_code = log.select(F.max(\"job_code\")).first()[0]\n",
    "\n",
    "# print(f\"min new job id：{min_job_code}\")\n",
    "# print(f\"max new job id：{max_job_code}\")\n",
    "\n",
    "log_train = log_train.groupBy(\"geek_id\", \"group_id\").agg(\n",
    "    F.collect_list(\"job_id\").alias(\"job_id_list\"),\n",
    "    F.collect_list(\"deal_type\").alias(\"deal_type_list\"),\n",
    "    F.collect_list(\"deal_time\").alias(\"deal_time_list\"),\n",
    "    F.collect_list(\"date\").alias(\"date_list\"),\n",
    "    F.collect_list(\"pred_person\").alias(\"pred_person_list\"),\n",
    "    F.collect_list(\"pred_job\").alias(\"pred_job_list\")\n",
    ").drop(\"group_id\").withColumn(\"list_size\", F.size(\"job_id_list\"))\n",
    "log_train.show()\n",
    "\n",
    "# For testing data\n",
    "\n",
    "\n",
    "\n",
    "windowSpec = Window.partitionBy(\"geek_id\").orderBy(\"deal_time\")\n",
    "log_test = log_test.withColumn(\"prev_deal_time\", F.lag(\"deal_time\").over(windowSpec))\n",
    "log_test = log_test.withColumn(\"time_diff\", F.when(F.isnull(log_test.deal_time - log_test.prev_deal_time), 0)\n",
    "                                  .otherwise(log_test.deal_time - log_test.prev_deal_time))\n",
    "\n",
    "log_test = log_test.withColumn(\"new_record\", F.when(log_test.time_diff > ddays * 86400, 1).otherwise(0))\n",
    "\n",
    "log_test = log_test.withColumn(\"group_id\", F.sum(\"new_record\").over(windowSpec))\n",
    "\n",
    "# from pyspark.ml.feature import StringIndexerModel\n",
    "# inputs1 = [\"job_id\"]\n",
    "# outputs1 = [\"job_code\"]\n",
    "# if os.path.exists(f'{file_path}/sparksteps/models/StringIndexer_prepare'):\n",
    "#     model1 = StringIndexerModel.load(f'{file_path}/sparksteps/models/StringIndexer_prepare')\n",
    "# log_test = model1.transform(log_test)\n",
    "\n",
    "log_test = log_test.groupBy(\"geek_id\", \"group_id\").agg(\n",
    "    F.collect_list(\"job_id\").alias(\"job_id_list\"),\n",
    "    F.collect_list(\"deal_type\").alias(\"deal_type_list\"),\n",
    "    F.collect_list(\"deal_time\").alias(\"deal_time_list\"),\n",
    "    F.collect_list(\"date\").alias(\"date_list\"),\n",
    "    F.collect_list(\"pred_person\").alias(\"pred_person_list\"),\n",
    "    F.collect_list(\"pred_job\").alias(\"pred_job_list\")\n",
    ").drop(\"group_id\").withColumn(\"list_size\", F.size(\"job_id_list\"))\n",
    "log_test.show()\n",
    "\n",
    "\n",
    "# For training data\n",
    "log_train.write.mode('overwrite').parquet(f'file:////root/target_dir/JobRec-main/sparksteps/train/log_res')\n",
    "print('log train cnt:', log_train.count())\n",
    "\n",
    "# For testing data\n",
    "log_test.write.mode('overwrite').parquet(f'file:////root/target_dir/JobRec-main/sparksteps/test/log_res')\n",
    "print('log test cnt:', log_test.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280873ea-5697-407c-b0b7-e8260811ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pyarrow in /root/miniconda3/lib/python3.12/site-packages (19.0.1)\n",
      "Requirement already satisfied: fastparquet in /root/miniconda3/lib/python3.12/site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /root/miniconda3/lib/python3.12/site-packages (from fastparquet) (2.2.3)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.12/site-packages (from fastparquet) (2.1.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in /root/miniconda3/lib/python3.12/site-packages (from fastparquet) (2.9.1)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/lib/python3.12/site-packages (from fastparquet) (2024.10.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from fastparquet) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bfdade3-d633-45d6-a4b9-28690e55b4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates tain: ['2023-06-03', '2023-06-04', '2023-06-05', '2023-06-06', '2023-06-07', '2023-06-08', '2023-06-09', '2023-06-10', '2023-06-11', '2023-06-12', '2023-06-13', '2023-06-14', '2023-06-15', '2023-06-16', '2023-06-17', '2023-06-18', '2023-06-19', '2023-06-20', '2023-06-21', '2023-06-22', '2023-06-23', '2023-06-24', '2023-06-25']\n",
      "dates test: ['2023-06-26', '2023-06-27', '2023-06-28', '2023-06-29', '2023-06-30']\n",
      "count: 10725\n",
      "count: 4109\n",
      "count: 4670\n",
      "count: 1778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580151/1915265353.py:93: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pred_job_list = row[6]\n",
      "/tmp/ipykernel_580151/1915265353.py:94: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pred_person = row[5][0]\n",
      "/tmp/ipykernel_580151/1915265353.py:95: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  job_id_list = row[1]\n",
      "/tmp/ipykernel_580151/1915265353.py:96: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  geek_id = row[0]\n",
      "/tmp/ipykernel_580151/1915265353.py:97: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  date_list = row[4]\n",
      "/tmp/ipykernel_580151/1915265353.py:98: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  unixtime_list = row[3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580151/1915265353.py:118: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pred_job_list = row[6]\n",
      "/tmp/ipykernel_580151/1915265353.py:119: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pred_person = row[5][0]\n",
      "/tmp/ipykernel_580151/1915265353.py:120: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  job_id_list = row[1]\n",
      "/tmp/ipykernel_580151/1915265353.py:121: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  geek_id = row[0]\n",
      "/tmp/ipykernel_580151/1915265353.py:122: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  date_list = row[4]\n",
      "/tmp/ipykernel_580151/1915265353.py:123: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  unixtime_list = row[3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{0: 1, 1: 1, 2: 1, 3: 1, 4: 7, 5: 1, 6: 11, 7: 4, 8: 4, 9: 8, 10: 5, 11: 5, 12: 4, 13: 1, 14: 1, 15: 2, 16: 12, 17: 5, 18: 2, 19: 7, 20: 6, 21: 2, 22: 5, 23: 4, 24: 2, 25: 2, 26: 7, 27: 4, 28: 4, 29: 4, 30: 2, 31: 7, 32: 6, 33: 2, 34: 12, 35: 5, 36: 2, 37: 17, 38: 9, 39: 12, 40: 16, 41: 8, 42: 1, 43: 1, 44: 1, 45: 3, 46: 2, 47: 7, 48: 4, 49: 3, 50: 1, 51: 17, 52: 2, 53: 3, 54: 5, 56: 6, 57: 7, 58: 7, 59: 6, 60: 3, 61: 7, 62: 5, 63: 10, 64: 1, 65: 1, 66: 2, 67: 1, 68: 7, 69: 1, 70: 6, 71: 2, 72: 2, 73: 3, 74: 6, 75: 9, 76: 4, 77: 3, 78: 2, 79: 2, 81: 4, 82: 7, 83: 1, 84: 14, 85: 2, 86: 4, 87: 5, 88: 5, 89: 2, 90: 2, 91: 4, 92: 4, 93: 13, 94: 4, 95: 1, 96: 4, 97: 3, 98: 1, 99: 7, 100: 1, 101: 1, 102: 11, 103: 10, 104: 3, 105: 6, 106: 2, 107: 1, 108: 18, 109: 1, 110: 1, 111: 16, 112: 5, 113: 1, 114: 8, 115: 1, 116: 1, 117: 1, 118: 6, 119: 2, 120: 2, 121: 10, 122: 7, 123: 3, 124: 4, 125: 10, 126: 1, 127: 1, 128: 10, 129: 1, 130: 12, 131: 8, 132: 5, 134: 3, 135: 3, 136: 1, 137: 2, 138: 4, 139: 6, 141: 4, 142: 5, 143: 2, 144: 2, 145: 3, 146: 1, 147: 3, 148: 1, 149: 1, 150: 1, 151: 5, 152: 8, 153: 11, 154: 6, 155: 6, 156: 12, 157: 26, 159: 4, 160: 14, 161: 3, 162: 1, 163: 1, 164: 3, 165: 11, 166: 7, 167: 3, 168: 4, 169: 2, 170: 6, 171: 6, 172: 27, 173: 1, 174: 17, 175: 2, 176: 10, 177: 1, 178: 3, 179: 2, 180: 1, 181: 2, 183: 14, 184: 2, 185: 10, 186: 2, 187: 12, 188: 7, 189: 2, 190: 1, 191: 4, 192: 9, 193: 3, 194: 2, 195: 2, 196: 17, 197: 5, 198: 2, 199: 3, 200: 1, 201: 24, 202: 3, 203: 1, 204: 2, 205: 5, 206: 15, 207: 4, 208: 3, 209: 15, 210: 27, 212: 2, 214: 16, 215: 3, 217: 6, 218: 4, 219: 2, 220: 3, 221: 21, 222: 11, 223: 7, 224: 12, 225: 4, 226: 6, 227: 5, 228: 5, 229: 5, 230: 2, 231: 3, 232: 17, 233: 6, 234: 10, 235: 2, 236: 10, 237: 1, 238: 1, 239: 1, 240: 1, 241: 10, 242: 1, 243: 1, 244: 4, 245: 1, 246: 1, 247: 4, 248: 3, 249: 1, 250: 3, 252: 3, 253: 12, 254: 1, 255: 5, 256: 6, 257: 1, 259: 3, 260: 3, 261: 14, 262: 1, 263: 19, 264: 1, 265: 7, 266: 12, 267: 17, 268: 4, 269: 8, 270: 6, 271: 7, 272: 1, 273: 3, 274: 4, 275: 4, 276: 9, 277: 4, 278: 13, 279: 6, 280: 1, 281: 4, 282: 1, 283: 2, 284: 2, 285: 1, 286: 2, 287: 1, 288: 1, 289: 5, 290: 3, 291: 6, 292: 10, 293: 2, 294: 1, 295: 14, 296: 23, 297: 5, 298: 3, 299: 24, 300: 2, 301: 5, 302: 6, 303: 11, 304: 3, 305: 3, 307: 7, 308: 12, 309: 1, 310: 3, 311: 16, 312: 5, 313: 3, 314: 1, 315: 5, 316: 28, 317: 20, 319: 2, 320: 4, 321: 2, 322: 6, 323: 1, 324: 9, 325: 10, 326: 7, 327: 14, 328: 4, 329: 9, 330: 6, 331: 1, 332: 2, 333: 9, 334: 22, 335: 2, 336: 4, 337: 1, 338: 8, 339: 8, 340: 4, 341: 2, 342: 3, 343: 5, 344: 3, 345: 1, 346: 1, 347: 1, 348: 4, 349: 5, 350: 9, 351: 13, 352: 10, 353: 4, 354: 4, 355: 1, 356: 24, 357: 2, 358: 5, 359: 4, 360: 1, 362: 1, 363: 1, 364: 5, 365: 1, 366: 5, 367: 17, 368: 2, 369: 1, 370: 3, 371: 1, 372: 7, 373: 5, 374: 2, 375: 4, 376: 3, 377: 1, 379: 1, 380: 6, 381: 1, 382: 7, 383: 15, 386: 4, 387: 4, 388: 3, 389: 11, 391: 14, 392: 5, 394: 5, 395: 6, 396: 1, 397: 3, 398: 1, 399: 8, 400: 3, 401: 2, 402: 1, 403: 3, 404: 1, 405: 10, 406: 1, 407: 2, 408: 3, 410: 7, 411: 6, 412: 14, 413: 3, 414: 10, 415: 1, 416: 11, 417: 3, 418: 4, 420: 9, 421: 2, 422: 3, 424: 5, 425: 1, 426: 3, 427: 4, 428: 3, 429: 1, 430: 4, 431: 13, 432: 13, 433: 5, 434: 2, 435: 6, 436: 8, 437: 4, 438: 1, 439: 2, 440: 1, 442: 3, 443: 8, 444: 16, 445: 3, 446: 3, 447: 11, 448: 8, 449: 3, 450: 1, 451: 3, 452: 1, 453: 2, 454: 7, 455: 1, 456: 1, 457: 11, 458: 7, 459: 1, 460: 3, 461: 1, 462: 1, 463: 10, 464: 2, 465: 1, 466: 7, 467: 1, 468: 4, 469: 1, 470: 3, 471: 4, 472: 3, 473: 1, 474: 7, 475: 5, 476: 4, 477: 6, 478: 24, 479: 4, 480: 2, 481: 4, 482: 9, 484: 1, 485: 9, 486: 9, 487: 3, 488: 3, 489: 1, 490: 2, 491: 3, 492: 6, 493: 7, 494: 5, 495: 1, 496: 3, 497: 11, 498: 5, 499: 22, 500: 3, 501: 2, 502: 3, 504: 1, 505: 5, 506: 4, 507: 11, 508: 3, 509: 6, 510: 1, 511: 3, 512: 1, 513: 5, 514: 3, 515: 1, 516: 4, 517: 2, 518: 4, 519: 1, 520: 1, 521: 17, 522: 9, 524: 2, 525: 5, 526: 2, 527: 11, 528: 1, 529: 5, 530: 3, 531: 5, 532: 2, 533: 2, 536: 2, 537: 1, 538: 6, 539: 25, 540: 4, 541: 1, 542: 2, 543: 1, 544: 7, 545: 3, 546: 4, 547: 3, 548: 12, 549: 3, 550: 1, 551: 3, 552: 3, 554: 20, 555: 2, 556: 8, 557: 2, 558: 2, 559: 9, 560: 7, 561: 1, 562: 6, 563: 3, 564: 2, 565: 1, 566: 6, 567: 1, 568: 2, 569: 4, 570: 1, 571: 5, 572: 9, 573: 1, 574: 6, 575: 12, 576: 5, 577: 1, 578: 4, 579: 1, 580: 1, 581: 2, 582: 2, 583: 10, 584: 3, 585: 2, 586: 1, 587: 2, 589: 6, 591: 2, 592: 12, 593: 20, 594: 6, 595: 3, 596: 13, 597: 2, 598: 6, 599: 1, 601: 2, 602: 11, 604: 3, 605: 3, 606: 5, 607: 3, 608: 4, 609: 1, 610: 1, 611: 4, 612: 3, 613: 4, 614: 5, 615: 6, 616: 1, 617: 5, 618: 6, 619: 13, 620: 1, 621: 4, 622: 2, 623: 7, 624: 4, 625: 11, 626: 7, 627: 4, 628: 16, 629: 3, 630: 2, 631: 6, 632: 8, 633: 1, 634: 2, 635: 3, 636: 1, 637: 3, 638: 1, 639: 3, 640: 8, 641: 2, 642: 1, 643: 8, 644: 1, 645: 3, 646: 2, 647: 2, 648: 4, 649: 3, 650: 1, 651: 5, 652: 1, 653: 4, 654: 2, 655: 2, 656: 6, 657: 16, 658: 7, 659: 3, 660: 8, 661: 4, 662: 7, 664: 2, 665: 7, 666: 6, 667: 20, 668: 3, 669: 2, 670: 8, 671: 4, 672: 5, 673: 3, 674: 5, 675: 1, 677: 1, 678: 4, 679: 6, 680: 1, 681: 5, 682: 2, 683: 1, 685: 4, 686: 20, 687: 5, 688: 9, 689: 8, 690: 1, 691: 1, 692: 8, 693: 1, 694: 1, 695: 2, 697: 6, 698: 4, 699: 5, 700: 3, 701: 3, 702: 3, 703: 10, 704: 4, 705: 5, 706: 3, 707: 9, 709: 1, 710: 1, 711: 4, 712: 14, 713: 8, 715: 4, 716: 9, 717: 2, 718: 7, 719: 2, 720: 11, 721: 10, 722: 8, 723: 6, 724: 1, 725: 5, 726: 2, 727: 11, 728: 1, 729: 3, 730: 7, 731: 2, 732: 3, 733: 4, 734: 3, 735: 8, 736: 8, 737: 1, 738: 2, 739: 5, 740: 12, 741: 1, 742: 4, 743: 1, 744: 1, 745: 1, 746: 8, 747: 4, 749: 1, 750: 17, 751: 2, 752: 3, 753: 2, 754: 4, 755: 2, 756: 1, 757: 7, 758: 2, 759: 6, 760: 3, 761: 18, 762: 3, 763: 3, 764: 2, 765: 6, 766: 1, 767: 1, 768: 1, 769: 4, 770: 3, 772: 1, 773: 5, 774: 2, 775: 2, 776: 5, 777: 1, 778: 3, 779: 2, 780: 4, 781: 1, 782: 1, 783: 2, 784: 6, 785: 3, 786: 10, 787: 6, 788: 2, 789: 3, 790: 6, 791: 7, 792: 5, 793: 2, 794: 1, 795: 1, 797: 7, 798: 6, 799: 1, 800: 4, 801: 1, 802: 9, 803: 4, 804: 2, 806: 6, 807: 1, 808: 5, 809: 4, 810: 2, 811: 2, 812: 2, 813: 2, 814: 10, 815: 4, 816: 1, 817: 14, 818: 3, 819: 4, 820: 29, 821: 1, 822: 7, 823: 2, 824: 3, 825: 2, 826: 3, 827: 3, 828: 11, 829: 3, 830: 3, 831: 7, 832: 4, 833: 11, 834: 12, 835: 1, 836: 5, 837: 6, 838: 2, 839: 5, 840: 3, 841: 11, 842: 1, 844: 12, 845: 1, 846: 4, 847: 5, 848: 4, 849: 5, 850: 1, 851: 4, 853: 1, 854: 2, 855: 1, 856: 14, 857: 3, 858: 3, 859: 1, 860: 4, 861: 4, 862: 1, 864: 2, 865: 9, 866: 12, 867: 15, 868: 2, 869: 1, 870: 3, 871: 1, 872: 5, 873: 1, 874: 5, 875: 2, 876: 3, 877: 5, 878: 6, 879: 4, 880: 1, 881: 3, 882: 1, 883: 5, 884: 5, 885: 6, 886: 5, 887: 1, 888: 2, 890: 5, 891: 11, 892: 8, 893: 4, 894: 1, 895: 2, 896: 8, 897: 9, 898: 16, 899: 1, 900: 4, 901: 3, 902: 3, 903: 2, 904: 1, 905: 2, 906: 9, 907: 1, 908: 6, 909: 5, 910: 3, 911: 8, 912: 6, 913: 2, 914: 2, 915: 1, 917: 2, 918: 1, 919: 2, 920: 1, 921: 9, 922: 29, 923: 1, 924: 1, 925: 1, 926: 1, 927: 10, 928: 4, 929: 1, 930: 15, 931: 2, 932: 11, 933: 2, 934: 3, 935: 2, 936: 1, 937: 2, 938: 4, 939: 1, 940: 3, 941: 2, 942: 3, 943: 1, 944: 4, 945: 6, 946: 4, 947: 2, 948: 2, 949: 2, 950: 7, 951: 7, 952: 15, 953: 1, 954: 11, 955: 5, 956: 7, 957: 1, 959: 1, 960: 1, 961: 1, 963: 1, 964: 4, 965: 1, 966: 2, 967: 7, 968: 2, 969: 6, 970: 1, 971: 4, 972: 3, 973: 3, 974: 7, 975: 1, 976: 2, 977: 4, 979: 5, 980: 15, 981: 17, 982: 2, 983: 4, 985: 6, 986: 8, 987: 2, 988: 1, 989: 20, 990: 1, 991: 1, 992: 7, 993: 3, 994: 5, 995: 3, 996: 4, 997: 4, 998: 1, 999: 2}\n",
      "count: 948\n",
      "{0: 1, 1: 1, 4: 3, 5: 1, 6: 6, 7: 1, 8: 2, 9: 6, 12: 3, 15: 1, 16: 4, 19: 2, 20: 2, 22: 3, 23: 3, 28: 1, 29: 3, 30: 2, 31: 5, 32: 3, 34: 4, 35: 3, 36: 1, 37: 3, 38: 1, 39: 6, 40: 5, 41: 2, 42: 1, 43: 1, 46: 1, 47: 3, 48: 2, 49: 1, 51: 10, 53: 4, 54: 3, 56: 1, 58: 5, 59: 1, 60: 1, 61: 2, 62: 4, 63: 6, 65: 1, 66: 1, 67: 1, 68: 2, 71: 1, 72: 1, 73: 2, 74: 3, 75: 1, 77: 1, 78: 1, 79: 1, 81: 2, 82: 4, 83: 1, 84: 3, 85: 3, 86: 3, 88: 2, 91: 3, 92: 3, 93: 7, 94: 3, 96: 2, 97: 1, 99: 4, 102: 5, 103: 2, 104: 1, 105: 4, 106: 2, 108: 6, 109: 1, 111: 5, 112: 2, 115: 1, 117: 1, 118: 2, 119: 2, 120: 1, 121: 3, 122: 3, 123: 2, 124: 3, 125: 3, 126: 2, 128: 4, 129: 1, 130: 5, 131: 6, 132: 2, 134: 1, 135: 3, 138: 1, 139: 2, 141: 1, 142: 1, 145: 1, 148: 1, 151: 3, 152: 3, 153: 8, 154: 2, 156: 9, 157: 7, 159: 5, 160: 6, 161: 1, 162: 1, 163: 1, 164: 1, 165: 4, 166: 1, 167: 1, 170: 3, 171: 3, 172: 21, 174: 6, 175: 2, 176: 3, 177: 1, 178: 1, 182: 1, 183: 2, 184: 4, 185: 5, 186: 3, 187: 3, 188: 2, 189: 1, 191: 2, 192: 2, 193: 1, 194: 1, 195: 2, 196: 8, 197: 1, 198: 1, 199: 1, 201: 5, 202: 2, 206: 8, 207: 3, 208: 1, 209: 5, 210: 16, 214: 2, 215: 1, 217: 3, 218: 2, 221: 5, 222: 2, 224: 9, 225: 2, 226: 4, 228: 2, 230: 1, 231: 2, 232: 9, 233: 4, 234: 4, 235: 1, 236: 6, 241: 2, 243: 1, 244: 1, 247: 3, 252: 1, 253: 5, 254: 1, 256: 3, 257: 1, 259: 2, 260: 1, 261: 4, 262: 1, 263: 5, 264: 1, 265: 1, 266: 5, 267: 11, 269: 8, 270: 3, 271: 1, 272: 1, 273: 1, 274: 1, 275: 3, 276: 3, 277: 1, 278: 5, 279: 1, 280: 1, 281: 1, 284: 3, 285: 1, 287: 1, 288: 1, 291: 4, 292: 5, 293: 1, 295: 3, 296: 13, 297: 2, 298: 3, 299: 5, 300: 1, 301: 2, 302: 2, 303: 3, 304: 3, 307: 3, 308: 2, 309: 1, 310: 1, 311: 8, 312: 1, 315: 1, 316: 11, 317: 8, 321: 2, 324: 2, 325: 3, 326: 2, 327: 1, 328: 1, 329: 1, 330: 4, 332: 2, 333: 3, 334: 6, 335: 2, 336: 3, 337: 1, 338: 5, 339: 2, 340: 1, 341: 2, 342: 1, 346: 1, 347: 1, 349: 5, 350: 5, 351: 5, 352: 4, 353: 6, 355: 1, 356: 9, 360: 1, 362: 1, 364: 1, 367: 11, 368: 1, 369: 1, 370: 1, 372: 4, 373: 2, 374: 2, 376: 1, 377: 1, 379: 1, 380: 2, 382: 3, 383: 1, 386: 3, 387: 1, 388: 2, 389: 3, 391: 4, 392: 2, 394: 1, 395: 1, 397: 4, 399: 4, 400: 1, 401: 1, 403: 2, 404: 1, 405: 5, 408: 2, 410: 3, 411: 2, 412: 3, 413: 2, 414: 5, 415: 1, 416: 7, 420: 2, 422: 2, 424: 1, 426: 1, 427: 1, 431: 3, 432: 3, 433: 1, 435: 5, 436: 4, 439: 1, 440: 1, 442: 2, 443: 1, 444: 5, 445: 1, 446: 2, 447: 4, 448: 3, 449: 1, 450: 1, 451: 1, 452: 1, 453: 1, 454: 3, 456: 1, 457: 3, 458: 1, 460: 1, 463: 4, 465: 1, 466: 2, 467: 1, 471: 3, 472: 1, 474: 3, 475: 4, 476: 1, 477: 2, 478: 7, 480: 2, 481: 2, 482: 5, 483: 1, 485: 4, 486: 4, 487: 1, 488: 1, 489: 1, 490: 3, 491: 2, 492: 2, 493: 4, 494: 5, 495: 1, 497: 3, 498: 3, 499: 13, 501: 1, 502: 3, 503: 1, 505: 2, 507: 3, 508: 1, 509: 1, 510: 1, 511: 1, 512: 2, 513: 3, 514: 1, 515: 1, 517: 1, 518: 3, 519: 1, 521: 8, 522: 6, 524: 1, 525: 4, 526: 1, 527: 2, 529: 2, 530: 1, 531: 3, 534: 1, 537: 1, 538: 3, 539: 7, 540: 3, 542: 1, 544: 4, 546: 1, 547: 2, 548: 4, 549: 2, 550: 1, 552: 2, 554: 14, 555: 1, 556: 3, 557: 1, 558: 1, 559: 3, 560: 1, 562: 1, 563: 2, 564: 1, 567: 1, 569: 3, 571: 3, 572: 4, 574: 3, 575: 3, 576: 1, 577: 1, 578: 1, 580: 1, 582: 1, 583: 3, 584: 2, 586: 1, 589: 2, 592: 7, 593: 6, 594: 2, 595: 2, 596: 5, 597: 1, 598: 2, 599: 1, 602: 3, 604: 1, 605: 2, 607: 1, 608: 1, 611: 1, 612: 1, 614: 5, 615: 1, 616: 1, 617: 1, 618: 1, 619: 4, 621: 3, 623: 4, 624: 2, 626: 2, 627: 2, 628: 6, 631: 2, 632: 2, 634: 1, 635: 2, 638: 1, 639: 1, 640: 3, 641: 1, 643: 2, 645: 1, 646: 1, 647: 1, 648: 2, 649: 1, 651: 4, 655: 1, 656: 3, 657: 7, 658: 2, 660: 5, 661: 1, 662: 4, 665: 4, 666: 1, 667: 10, 669: 2, 670: 3, 671: 1, 673: 1, 674: 3, 675: 1, 677: 1, 678: 1, 679: 3, 681: 2, 682: 2, 686: 5, 687: 1, 688: 3, 689: 2, 692: 4, 695: 1, 696: 1, 697: 2, 699: 2, 701: 1, 702: 1, 703: 2, 704: 1, 707: 1, 709: 1, 711: 2, 712: 4, 713: 3, 715: 6, 716: 2, 717: 1, 718: 3, 719: 1, 720: 4, 721: 2, 722: 4, 723: 1, 725: 2, 727: 3, 728: 1, 730: 3, 731: 2, 732: 1, 734: 1, 735: 7, 737: 1, 739: 2, 740: 8, 741: 1, 742: 1, 745: 1, 746: 3, 747: 2, 750: 5, 753: 1, 754: 2, 755: 1, 757: 3, 759: 3, 760: 1, 761: 1, 765: 3, 766: 1, 768: 1, 769: 1, 771: 1, 773: 3, 774: 1, 775: 4, 776: 2, 780: 3, 782: 1, 783: 1, 784: 3, 785: 2, 787: 2, 789: 3, 790: 4, 791: 2, 793: 1, 795: 1, 797: 1, 798: 1, 800: 1, 801: 3, 802: 3, 803: 1, 804: 1, 806: 1, 807: 1, 808: 2, 809: 3, 810: 1, 811: 1, 812: 1, 814: 1, 815: 3, 816: 1, 817: 4, 818: 3, 819: 1, 820: 10, 822: 3, 823: 2, 824: 3, 825: 1, 826: 2, 827: 2, 828: 3, 829: 4, 830: 1, 831: 4, 832: 4, 833: 5, 834: 8, 835: 1, 836: 1, 837: 3, 838: 1, 839: 2, 841: 4, 844: 3, 846: 4, 847: 2, 848: 1, 849: 2, 853: 2, 856: 7, 857: 1, 859: 1, 861: 1, 864: 1, 865: 3, 866: 2, 867: 8, 872: 1, 874: 1, 875: 3, 876: 1, 877: 4, 878: 5, 879: 3, 880: 1, 883: 2, 884: 2, 885: 2, 886: 1, 887: 1, 888: 1, 890: 3, 891: 6, 892: 4, 893: 1, 894: 1, 895: 1, 896: 2, 897: 6, 898: 4, 899: 1, 900: 1, 905: 2, 906: 1, 908: 2, 909: 2, 911: 4, 913: 1, 915: 1, 919: 1, 920: 1, 921: 5, 922: 18, 924: 1, 927: 5, 928: 1, 930: 4, 931: 1, 932: 4, 934: 1, 935: 1, 937: 1, 938: 3, 940: 2, 941: 1, 942: 3, 943: 1, 944: 1, 945: 1, 946: 3, 947: 1, 948: 1, 949: 2, 950: 4, 951: 1, 952: 1, 954: 2, 955: 1, 956: 1, 957: 1, 960: 2, 964: 1, 967: 3, 969: 2, 971: 1, 972: 1, 973: 3, 974: 2, 975: 1, 977: 2, 978: 1, 979: 3, 980: 6, 981: 9, 982: 1, 985: 2, 986: 1, 987: 1, 989: 9, 990: 1, 991: 2, 992: 4, 993: 1, 994: 2, 995: 1, 996: 1, 999: 3}\n",
      "count: 690\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import datetime \n",
    "\n",
    "# 模拟命令行参数\n",
    "sys.argv = [\n",
    "     \"JobRec/sparksteps/1.py\",  # 脚本名称\n",
    "    \"2023-06-03\",  # start_date\n",
    "    \"2023-06-30\",  # end_date\n",
    "    \"/root/target_dir/JobRec-main\",  # path\n",
    "    \"bj\"  # city_short\n",
    "]\n",
    "\n",
    "# 以下是 5.py 脚本的其余代码\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"xxx\"\n",
    "\n",
    "ddays = 7\n",
    "LIST_CNT = 30\n",
    "\n",
    "K_job = 500\n",
    "K_person = 1000  # number of person classes ### same as the K in 16.data_xxx.ipynb\n",
    "numTopics = 1000  \n",
    "test_days = 5\n",
    "POS_CODE_LOW = '100000'\n",
    "POS_CODE_HIGH = '110000'\n",
    "start_date = sys.argv[1]\n",
    "end_date = sys.argv[2]\n",
    "path = sys.argv[3]\n",
    "file_path = 'file:///' + path\n",
    "city_short = sys.argv[4]\n",
    "\n",
    "def generate_train_test_dates(start_date, end_date):\n",
    "    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    date_list_train = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range(0, (end - start).days + 1 - test_days)]\n",
    "    date_list_test = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range((end - start).days + 1 - test_days, (end - start).days + 1)]\n",
    "    \n",
    "    return date_list_train, date_list_test\n",
    "\n",
    "dates_train, dates_test = generate_train_test_dates(start_date, end_date)\n",
    "print('dates tain:', dates_train)\n",
    "print('dates test:', dates_test)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"spark_data_query\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",500)\\\n",
    "    .config(\"spark.driver.memory\", \"32g\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# For training data\n",
    "folder_path_train = f'{path}/sparksteps/train/log_res'\n",
    "parquet_files = [os.path.join(folder_path_train, f) for f in os.listdir(folder_path_train) if f.endswith('.parquet')]\n",
    "df_train = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n",
    "# display(df_train)\n",
    "print('count:', df_train.shape[0])\n",
    "\n",
    "# For testing data\n",
    "folder_path_test = f'{path}/sparksteps/test/log_res'\n",
    "parquet_files = [os.path.join(folder_path_test, f) for f in os.listdir(folder_path_test) if f.endswith('.parquet')]\n",
    "df_test = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n",
    "# display(df_test)\n",
    "print('count:', df_test.shape[0])\n",
    "\n",
    "# For training data\n",
    "df_train = df_train[df_train['list_size'] > LIST_CNT]\n",
    "# display(df_train)\n",
    "print('count:', df_train.shape[0])\n",
    "\n",
    "# For testing data\n",
    "df_test = df_test[df_test['list_size'] > LIST_CNT]\n",
    "# display(df_test)\n",
    "print('count:', df_test.shape[0])\n",
    "\n",
    "# For training data\n",
    "if not os.path.exists(f'{path}/sparksteps/train/relation'):\n",
    "    os.mkdir(f'{path}/sparksteps/train/relation')\n",
    "cnt_dict_train = {i:0 for i in range(K_person)}\n",
    "for index, row in df_train.iterrows():\n",
    "#     print(index, row)\n",
    "    pred_job_list = row[6]\n",
    "    pred_person = row[5][0]\n",
    "    job_id_list = row[1]\n",
    "    geek_id = row[0]\n",
    "    date_list = row[4]\n",
    "    unixtime_list = row[3]\n",
    "    cnt = cnt_dict_train[pred_person]\n",
    "#     print(str(pred_person) + ':'+str(cnt))\n",
    "    relation_file = open(f'{path}/sparksteps/train/relation/relation_'+str(pred_person)+'_'+str(cnt)+'.csv', 'w')\n",
    "    relation_file.write('id,pred_job,unix_time,date,job_id,geek_id\\n')\n",
    "    relation_file.close()\n",
    "    relation_file = open(f'{path}/sparksteps/train/relation/relation_'+str(pred_person)+'_'+str(cnt)+'.csv', 'a')\n",
    "    for i in range(len(pred_job_list)):\n",
    "        relation_file.write(str(i)+','+str(pred_job_list[i])+','+str(unixtime_list[i])+','+str(date_list[i])+','+str(job_id_list[i])+','+str(geek_id)+'\\n')\n",
    "    relation_file.close()\n",
    "    cnt_dict_train[pred_person] = cnt + 1\n",
    "    \n",
    "print(len(cnt_dict_train))\n",
    "\n",
    "# For testing data\n",
    "if not os.path.exists(f'{path}/sparksteps/test/relation'):\n",
    "    os.mkdir(f'{path}/sparksteps/test/relation')\n",
    "cnt_dict_test = {i:0 for i in range(K_person)}\n",
    "for index, row in df_test.iterrows():\n",
    "#     print(index, row)\n",
    "    pred_job_list = row[6]\n",
    "    pred_person = row[5][0]\n",
    "    job_id_list = row[1]\n",
    "    geek_id = row[0]\n",
    "    date_list = row[4]\n",
    "    unixtime_list = row[3]\n",
    "    cnt = cnt_dict_test[pred_person]\n",
    "#     print(str(pred_person) + ':'+str(cnt))\n",
    "    relation_file = open(f'{path}/sparksteps/test/relation/relation_'+str(pred_person)+'_'+str(cnt)+'.csv', 'w')\n",
    "    relation_file.write('id,pred_job,unix_time,date,job_id,geek_id\\n')\n",
    "    relation_file.close()\n",
    "    relation_file = open(f'{path}/sparksteps/test/relation/relation_'+str(pred_person)+'_'+str(cnt)+'.csv', 'a')\n",
    "    for i in range(len(pred_job_list)):\n",
    "        relation_file.write(str(i)+','+str(pred_job_list[i])+','+str(unixtime_list[i])+','+str(date_list[i])+','+str(job_id_list[i])+','+str(geek_id)+'\\n')\n",
    "    relation_file.close()\n",
    "    cnt_dict_test[pred_person] = cnt + 1\n",
    "    \n",
    "print(len(cnt_dict_test))\n",
    "\n",
    "# For training data\n",
    "new_dict = {key: value for key, value in cnt_dict_train.items() if value != 0}\n",
    "print(new_dict)\n",
    "print('count:', len(new_dict))\n",
    "\n",
    "# For testing data\n",
    "new_dict = {key: value for key, value in cnt_dict_test.items() if value != 0}\n",
    "print(new_dict)\n",
    "print('count:', len(new_dict))\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "# For training data\n",
    "with open(f'{path}/sparksteps/train/relation/dict.pkl', 'wb') as f:\n",
    "    pkl.dump(cnt_dict_train, f)\n",
    "\n",
    "# For testing data\n",
    "with open(f'{path}/sparksteps/test/relation/dict.pkl', 'wb') as f:\n",
    "    pkl.dump(cnt_dict_test, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63967271-2272-42f4-beea-5d3fb29c74e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates tain: ['2023-06-03', '2023-06-04', '2023-06-05', '2023-06-06', '2023-06-07', '2023-06-08', '2023-06-09', '2023-06-10', '2023-06-11', '2023-06-12', '2023-06-13', '2023-06-14', '2023-06-15', '2023-06-16', '2023-06-17', '2023-06-18', '2023-06-19', '2023-06-20', '2023-06-21', '2023-06-22', '2023-06-23', '2023-06-24', '2023-06-25']\n",
      "dates test: ['2023-06-26', '2023-06-27', '2023-06-28', '2023-06-29', '2023-06-30']\n",
      "count train: 1000\n",
      "person 0 : 1\n",
      "\t 34706274\n",
      "person 100 : 1\n",
      "\t 529034633\n",
      "person 200 : 1\n",
      "\t 547096422\n",
      "person 300 : 2\n",
      "\t 38100022\n",
      "\t 29809914\n",
      "person 400 : 3\n",
      "\t 103725393\n",
      "\t 82804470\n",
      "\t 550102510\n",
      "person 500 : 3\n",
      "\t 72193900\n",
      "\t 32261401\n",
      "\t 589703810\n",
      "person 600 : 0\n",
      "person 700 : 3\n",
      "\t 45410519\n",
      "\t 3025525\n",
      "\t 77082294\n",
      "person 800 : 4\n",
      "\t 54038196\n",
      "\t 38791694\n",
      "\t 504576735\n",
      "\t 39240471\n",
      "person 900 : 4\n",
      "\t 80525208\n",
      "\t 67749150\n",
      "\t 35366431\n",
      "\t 45628204\n",
      "person 0 : 1\n",
      "\t 39472752 , [2, 2, 2, 2, 2, 37, 87, 291, 451, 451, 320, 0, 1, 2, 2, 2, 2, 5, 87, 138, 320, 320, 338, 0, 2, 451, 0, 2, 2, 2, 78, 87, 138, 451, 0, 1, 0, 1, 2, 2, 2, 2, 5, 37, 138, 320, 320, 320, 138, 454, 0, 2, 2, 2, 2, 2, 2, 2, 56, 451, 5, 0] , [0, 0, 2, 2, 2, 2, 2, 2, 5, 37, 37, 87, 87, 138, 354, 138, 451, 87, 354, 35]\n",
      "person 100 : 0\n",
      "person 200 : 0\n",
      "person 300 : 1\n",
      "\t 29809914 , [367, 200, 36, 2, 2, 2, 2, 2, 56, 87, 200, 221, 238, 238, 320, 320, 0, 1, 2, 2, 78, 205, 205, 238, 338, 1, 1, 2, 2, 2, 2, 78, 94, 238, 273, 320, 2, 2, 2, 2, 87, 283, 283, 320, 367, 480, 71, 1, 2, 2, 2, 71, 78, 200, 273, 273, 320, 2, 2, 2, 2, 2, 2, 2, 94, 200, 320, 480, 2, 2, 214, 2, 0, 1, 465, 1, 2, 2, 238, 320, 338, 456, 470, 1, 2, 2, 2, 2, 78, 87, 94, 2, 2, 35, 56, 71, 78, 78, 200, 200, 2, 2, 2, 2, 2, 36, 78, 200, 221, 270, 320, 0, 2, 2, 2, 2, 2, 36, 56, 87, 138, 171, 200, 320, 0, 2, 2, 2, 2, 78, 87, 265, 380, 0, 2, 2, 2, 2, 2, 2, 2, 87, 320, 451, 2, 2, 2, 2, 2, 2, 2, 2, 87, 92, 320, 138, 0, 1, 2, 2, 2, 2, 71, 291, 319, 320, 273, 1, 2, 2, 2, 36, 71, 71, 78, 138, 320, 338, 1, 2, 0, 2, 2, 2, 2, 2, 36, 56, 200, 311, 380, 320, 1, 0, 2, 2, 2, 77, 117, 154, 320, 367, 1, 2, 2, 2, 2, 2, 71, 71, 78, 87, 200, 273, 2, 2, 2, 2, 2, 71, 78, 154, 200, 200, 2, 2, 2] , [2, 2, 2, 71, 78, 78, 87, 154, 238, 320, 480, 1, 484, 311, 87, 2, 2, 1, 1, 1]\n",
      "person 400 : 1\n",
      "\t 46203150 , [4, 43, 44, 70, 95, 155, 198, 198, 247, 250, 250, 272, 290, 337, 349, 386, 388, 104, 4, 4, 4, 4, 44, 104, 148, 148, 172, 172, 172, 172, 196, 196, 203, 226, 247, 250, 250, 363, 104, 104, 116, 116, 92, 424, 249, 249, 10, 172, 74, 172, 18, 43, 198, 249, 249, 116, 10, 43, 74, 74, 172, 172, 172, 203, 203, 203, 226, 249, 249, 249, 292, 337, 438, 438, 74, 352, 292, 249, 160, 424, 290] , [203, 172, 43, 172, 437, 198, 249, 172, 43, 37, 369, 130, 202, 141, 203, 292, 74, 424, 172, 203]\n",
      "person 500 : 0\n",
      "person 600 : 0\n",
      "person 700 : 0\n",
      "person 800 : 1\n",
      "\t 38791694 , [35, 35, 53, 77, 136, 138, 171, 399, 482, 35, 35, 56, 87, 136, 200, 200, 350, 484, 2, 35, 35, 35, 35, 200, 200, 367, 396, 451, 35, 35, 78, 35, 0, 2, 23, 35, 56, 87, 138, 138, 200, 200, 238, 273, 320, 23, 26, 484, 2, 56, 56, 78, 78, 87, 157, 171, 200, 200, 320, 0, 1, 35, 35, 35, 56, 77, 200, 423, 484, 0, 35, 78, 78, 87, 200, 221, 320, 350, 396, 425, 484, 451, 56] , [2, 0, 0, 2, 35, 78, 78, 122, 138, 200, 200, 200, 320, 320, 396, 35, 0, 48, 291, 221]\n",
      "person 900 : 1\n",
      "\t 36942960 , [17, 17, 54, 57, 121, 127, 149, 196, 250, 280, 280, 285, 306, 57, 121, 121, 17, 57, 92, 121, 122, 124, 124, 127, 127, 280, 311, 329, 329, 121, 121, 329, 275, 0, 1, 2, 2, 78, 124, 238, 320, 320, 353, 412, 17, 121, 121, 121, 124, 149, 257, 285, 285, 317, 200, 35, 56, 57, 57, 87, 87, 92, 171, 221, 285, 386, 17, 56, 57, 124, 149, 155, 157, 249, 278, 280, 291, 386, 386, 250, 17, 57, 78, 87, 92, 149, 157, 207, 280, 280, 285, 285, 306, 317, 421, 250, 484, 26, 57, 57, 78, 124, 280, 317, 77, 122, 200, 26, 54, 57, 87, 92, 121, 121, 124, 250, 320, 386, 48, 122, 285, 329, 124, 329, 2, 35, 48, 122, 124, 149, 149, 157, 196, 280, 280, 280, 329, 35, 54, 57, 92, 121, 121, 200, 280, 280, 280, 309, 350, 399, 0, 0, 9, 48, 121, 124, 207, 280, 306, 306, 322, 451, 17, 56, 64, 124, 200, 280, 285, 367, 379, 22, 35, 57, 87, 92, 107, 149, 149, 280, 285, 2, 17, 52, 56, 57, 57, 78, 87, 124, 124, 155, 178, 280, 17, 35, 56, 57, 64, 78, 87, 121, 124, 155, 280, 9, 52, 54, 124, 149, 149, 160, 213, 250, 280, 285, 320, 412, 247, 26, 57, 57, 78, 157, 280, 280, 280, 384, 386, 250, 17, 57, 78, 104, 201, 250, 155, 448, 388, 134, 137, 57, 39, 54, 280, 5, 216, 35, 292, 148, 376, 196, 386, 279, 363, 172, 10, 249, 409, 148, 10, 196, 457, 299, 148, 198, 155, 249, 4, 172, 249, 470, 104, 349, 292, 127, 92, 4, 198, 249, 249, 429, 299, 118, 120, 460] , [457, 155, 249, 280, 79, 285, 148, 57, 414, 87, 438, 141, 2, 299, 2, 87, 280, 424, 201, 161]\n",
      "total train sample count: 4670\n",
      "total test sample count: 1778\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "\n",
    "# 模拟命令行参数\n",
    "sys.argv = [\n",
    "     \"JobRec/sparksteps/1.py\",  # 脚本名称\n",
    "    \"2023-06-03\",  # start_date\n",
    "    \"2023-06-30\",  # end_date\n",
    "    \"/root/target_dir/JobRec-main\",  # path\n",
    "    \"bj\"  # city_short\n",
    "]\n",
    "\n",
    "# 以下是 6.py 脚本的其余代码\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"xxx\"\n",
    "\n",
    "\n",
    "ddays = 7\n",
    "LIST_CNT = 30\n",
    "TOP_K = 20\n",
    "\n",
    "K_job = 500\n",
    "K_person = 1000  # number of person classes ### same as the K in 16.data_xxx.ipynb\n",
    "numTopics = 1000  \n",
    "test_days = 5\n",
    "POS_CODE_LOW = '100000'\n",
    "POS_CODE_HIGH = '110000'\n",
    "start_date = sys.argv[1]\n",
    "end_date = sys.argv[2]\n",
    "path = sys.argv[3]\n",
    "file_path = 'file:///' + path\n",
    "city_short = sys.argv[4]\n",
    "\n",
    "\n",
    "def generate_train_test_dates(start_date, end_date):\n",
    "    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    date_list_train = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range(0, (end - start).days + 1 - test_days)]\n",
    "    date_list_test = [(start + datetime.timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "                 for x in range((end - start).days + 1 - test_days, (end - start).days + 1)]\n",
    "    \n",
    "    return date_list_train, date_list_test\n",
    "\n",
    "dates_train, dates_test = generate_train_test_dates(start_date, end_date)\n",
    "print('dates tain:', dates_train)\n",
    "print('dates test:', dates_test)\n",
    "\n",
    "# For training data\n",
    "dict_train = {}\n",
    "with open(f'{path}/sparksteps/train/relation/dict.pkl', 'rb') as f:\n",
    "    dict_train = pkl.load(f)\n",
    "print('count train:', len(dict_train))\n",
    "\n",
    "# For testing data\n",
    "dict_test = {}\n",
    "with open(f'{path}/sparksteps/test/relation/dict.pkl', 'rb') as f:\n",
    "    dict_test = pkl.load(f)\n",
    "    \n",
    "from scipy.sparse import coo_matrix, linalg\n",
    "\n",
    "def A_cal(row, col, values, N, M, deg_n, deg_e, w_e):\n",
    "    H = coo_matrix((values, (row, col)), shape=(N, M)).tocsc()\n",
    "    row1 = range(N)\n",
    "    col1 = range(N)\n",
    "    Dn = coo_matrix((deg_n, (row1, col1)), shape=(N, N)).tocsc()\n",
    "    deg_n_inv = []\n",
    "    for i in deg_n:\n",
    "        deg_n_inv.append(1.0/(i+0.000001))\n",
    "    Dn_inv = coo_matrix((deg_n_inv, (row1, col1)), shape=(N, N)).tocsc()\n",
    "    row2 = range(M)\n",
    "    col2 = range(M)\n",
    "    De = coo_matrix((deg_e, (row2, col2)), shape=(M, M)).tocsc()\n",
    "    deg_e_inv = []\n",
    "    for i in deg_e:\n",
    "        deg_e_inv.append(1.0/i)\n",
    "    De_inv = coo_matrix((deg_e_inv, (row2, col2)), shape=(M, M)).tocsc()\n",
    "    We = coo_matrix((w_e, (row2, col2)), shape=(M, M)).tocsc()\n",
    "    tmp = H.dot(We)\n",
    "    tmp2 = De_inv.dot(H.T)\n",
    "    A = tmp.dot(tmp2)\n",
    "#     print(A.shape)\n",
    "    return A\n",
    "\n",
    "# For training data\n",
    "kk = 0\n",
    "if not os.path.exists(f'{path}/sparksteps/train/sample'):\n",
    "    os.mkdir(f'{path}/sparksteps/train/sample')\n",
    "for a in range(K_person):\n",
    "    b = dict_train[a]\n",
    "    if a % 100 == 0:\n",
    "        print('person', a, ':', b)\n",
    "    if b == 0:\n",
    "        continue\n",
    "    \n",
    "    # b!=0\n",
    "    # cc = random.randint(0,b-1)\n",
    "    for cc in range(b):\n",
    "        \n",
    "        # row: different sessions for the same person class, col: item list\n",
    "        joblst = []\n",
    "        label = -1\n",
    "\n",
    "        job_relation_dict = {}\n",
    "        for i in range(K_job):\n",
    "            job_relation_dict[i] = []\n",
    "        \n",
    "        # two views of hypergraphs\n",
    "        # 同一用户class，不同session，每一session构成一条超边\n",
    "        H1_row = []\n",
    "        H1_col = []\n",
    "        H1_val = []\n",
    "        De1 = []\n",
    "        Dv1 = [0 for i in range(K_job)]\n",
    "\n",
    "        # 同一类用户class，当前Item对后一所有出现的Item构建超边\n",
    "        H2_row = []\n",
    "        H2_col = []\n",
    "        H2_val = []\n",
    "        De2 = []\n",
    "        Dv2 = [0 for i in range(K_job)]\n",
    "    \n",
    "        cnt = 0\n",
    "        geek_id = -1\n",
    "        for c in range(b):\n",
    "            file = f'{path}/sparksteps/train/relation/relation_'+str(a)+'_'+str(c)+'.csv'\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            job_list = df['pred_job'].values.tolist()\n",
    "            if cc == c:\n",
    "                geek_id = str(df['geek_id'].values[0])\n",
    "                label_job = job_list[-TOP_K:] \n",
    "                job_list = job_list[:-TOP_K]\n",
    "            joblst = job_list\n",
    "            label = label_job\n",
    "\n",
    "            tmp_lst = []\n",
    "            tmp_cnt = 0\n",
    "            last = -1\n",
    "\n",
    "            for jobID in job_list:\n",
    "                if last >= 0 and jobID not in job_relation_dict[last]:\n",
    "                    job_relation_dict[last].append(jobID)\n",
    "                last = jobID\n",
    "\n",
    "\n",
    "                if jobID not in tmp_lst:\n",
    "                    tmp_lst.append(jobID)\n",
    "                    tmp_cnt += 1\n",
    "                    Dv1[jobID] += 1\n",
    "                    H1_row.append(jobID)\n",
    "                    H1_col.append(cnt)\n",
    "                    H1_val.append(1.0)\n",
    "            De1.append(tmp_cnt)\n",
    "            cnt += 1\n",
    "\n",
    "        cnt = 0\n",
    "        for key, vallst in job_relation_dict.items():\n",
    "            for value in vallst:\n",
    "                H2_row.append(value)\n",
    "                H2_col.append(cnt)\n",
    "                H2_val.append(1.0)\n",
    "                Dv2[value] += 1\n",
    "            num = len(vallst)\n",
    "            if num > 0:\n",
    "                De2.append(num)\n",
    "                cnt += 1\n",
    "        N1 = len(Dv1)\n",
    "        # print('\\tN1:', N1)\n",
    "        M1 = len(De1)\n",
    "        # print('\\tM1:', M1)\n",
    "        We1 = []\n",
    "        for i in range(M1):\n",
    "            We1.append(1.0)\n",
    "\n",
    "        A1 = A_cal(H1_row, H1_col, H1_val, N1, M1, Dv1, De1, We1)\n",
    "\n",
    "\n",
    "        N2 = len(Dv2)\n",
    "        # print('\\tN2:', N2)\n",
    "        M2 = len(De2)\n",
    "        # print('\\tM2:', M2)\n",
    "        We2 = []\n",
    "        for i in range(M2):\n",
    "            We2.append(1.0)\n",
    "\n",
    "        A2 = A_cal(H2_row, H2_col, H2_val, N2, M2, Dv2, De2, We2)\n",
    "        clustercenters = []\n",
    "        with open(f'{path}/sparksteps/models/Clustercenters_occ.pkl', 'rb') as f:\n",
    "            clustercenters = pkl.load(f)\n",
    "        Fea = np.array(clustercenters)\n",
    "\n",
    "        with open(f'{path}/sparksteps/train/sample/datasample_'+str(kk)+'.pkl', 'wb') as f:\n",
    "            data = (A1, A2, Fea, joblst, label, geek_id)\n",
    "            if a % 100 == 0:\n",
    "                print('\\t', geek_id)\n",
    "            pkl.dump(data, f)\n",
    "            kk += 1\n",
    "kk_train = kk\n",
    "\n",
    "# For testing data\n",
    "kk = 0\n",
    "if not os.path.exists(f'{path}/sparksteps/test/sample'):\n",
    "    os.mkdir(f'{path}/sparksteps/test/sample')\n",
    "for a in range(K_person):\n",
    "    b = dict_test[a]\n",
    "    if a % 100 == 0:\n",
    "        print('person', a, ':', b)\n",
    "    if b == 0:\n",
    "        continue\n",
    "\n",
    "    # cc = random.randint(0,b-1)\n",
    "    for cc in range(b):\n",
    "        \n",
    "        # row: different sessions for the same person class, col: item list\n",
    "        joblst = []\n",
    "        label = -1\n",
    "\n",
    "        job_relation_dict = {}\n",
    "        for i in range(K_job):\n",
    "            job_relation_dict[i] = []\n",
    "        \n",
    "        # two views of hypergraphs\n",
    "        H1_row = []\n",
    "        H1_col = []\n",
    "        H1_val = []\n",
    "        De1 = []\n",
    "        Dv1 = [0 for i in range(K_job)]\n",
    "\n",
    "        H2_row = []\n",
    "        H2_col = []\n",
    "        H2_val = []\n",
    "        De2 = []\n",
    "        Dv2 = [0 for i in range(K_job)]\n",
    "    \n",
    "        cnt = 0\n",
    "        geek_id = -1\n",
    "        for c in range(b):\n",
    "            file = f'{path}/sparksteps/test/relation/relation_'+str(a)+'_'+str(c)+'.csv'\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            job_list = df['pred_job'].values.tolist()\n",
    "            if cc == c:\n",
    "                geek_id = str(df['geek_id'].values[0])\n",
    "                label_job = job_list[-TOP_K:] \n",
    "                job_list = job_list[:-TOP_K]\n",
    "            joblst = job_list\n",
    "            label = label_job\n",
    "\n",
    "            tmp_lst = []\n",
    "            tmp_cnt = 0\n",
    "            last = -1\n",
    "\n",
    "            for jobID in job_list:\n",
    "                if last >= 0 and jobID not in job_relation_dict[last]:\n",
    "                    job_relation_dict[last].append(jobID)\n",
    "                last = jobID\n",
    "\n",
    "\n",
    "                if jobID not in tmp_lst:\n",
    "                    tmp_lst.append(jobID)\n",
    "                    tmp_cnt += 1\n",
    "                    Dv1[jobID] += 1\n",
    "                    H1_row.append(jobID)\n",
    "                    H1_col.append(cnt)\n",
    "                    H1_val.append(1.0)\n",
    "            De1.append(tmp_cnt)\n",
    "            cnt += 1\n",
    "\n",
    "        cnt = 0\n",
    "        for key, vallst in job_relation_dict.items():\n",
    "            for value in vallst:\n",
    "                H2_row.append(value)\n",
    "                H2_col.append(cnt)\n",
    "                H2_val.append(1.0)\n",
    "                Dv2[value] += 1\n",
    "            num = len(vallst)\n",
    "            if num > 0:\n",
    "                De2.append(num)\n",
    "                cnt += 1\n",
    "\n",
    "\n",
    "        N1 = len(Dv1)\n",
    "        # print('\\tN1:', N1)\n",
    "        M1 = len(De1)\n",
    "        # print('\\tM1:', M1)\n",
    "        We1 = []\n",
    "        for i in range(M1):\n",
    "            We1.append(1.0)\n",
    "\n",
    "        A1 = A_cal(H1_row, H1_col, H1_val, N1, M1, Dv1, De1, We1)\n",
    "        # print('\\tA1 shape:', A1.shape)\n",
    "\n",
    "\n",
    "        N2 = len(Dv2)\n",
    "        # print('\\tN2:', N2)\n",
    "        M2 = len(De2)\n",
    "        # print('\\tM2:', M2)\n",
    "        We2 = []\n",
    "        for i in range(M2):\n",
    "            We2.append(1.0)\n",
    "\n",
    "        A2 = A_cal(H2_row, H2_col, H2_val, N2, M2, Dv2, De2, We2)    \n",
    "        # print('\\tA2 shape:', A2.shape)\n",
    "\n",
    "        # print('\\tjob list:', joblst)\n",
    "        # print('\\tjob list len:', len(joblst))\n",
    "        # print('\\tnext job ID:', label)\n",
    "\n",
    "        clustercenters = []\n",
    "        with open(f'{path}/sparksteps/models/Clustercenters_occ.pkl', 'rb') as f:\n",
    "            clustercenters = pkl.load(f)\n",
    "        Fea = np.array(clustercenters)\n",
    "        # print('\\tocc features shape:', Fea.shape)\n",
    "\n",
    "        with open(f'{path}/sparksteps/test/sample/datasample_'+str(kk)+'.pkl', 'wb') as f:\n",
    "            data = (A1, A2, Fea, joblst, label, geek_id)\n",
    "            if a % 100 == 0:\n",
    "                print('\\t', geek_id, ',', joblst, ',', label)\n",
    "            pkl.dump(data, f)\n",
    "            kk += 1\n",
    "kk_test = kk\n",
    "\n",
    "print('total train sample count:', kk_train)\n",
    "print('total test sample count:', kk_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd9219-64f2-46e3-a08d-59ad0735451f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobpy38",
   "language": "python",
   "name": "jobpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
